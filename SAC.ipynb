{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2601a6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devjac/Code/python/learn-pytorch/.venv/lib/python3.10/site-packages/gym/wrappers/monitoring/video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils.spawn\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import namedtuple, deque\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf90475",
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f9a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.3  # How much we value entropy / exploration, increasing this will increase exploration.\n",
    "GAMMA = 0.9  # How much we value future rewards.\n",
    "TAU = 0.01  # How much q_target is updated when polyak averaging (step 15).\n",
    "POLICY_LR = 0.001  # Policy learning rate.\n",
    "Q_LR = 0.001  # Q learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf163657",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89982995",
   "metadata": {},
   "outputs": [],
   "source": [
    "SARS = namedtuple('SARS', 'state, action, reward, next_state, t, failed, limit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2cd7e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652], dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, dtype=torch.float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=0)\n",
    "input = torch.tensor([1, 2, 3], dtype=float)\n",
    "display(input)\n",
    "output = softmax(input)\n",
    "display(output)\n",
    "sum(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10ba09d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [1., 2., 3.],\n",
       "        [3., 3., 3.]], dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0900, 0.2447, 0.6652],\n",
       "        [0.0900, 0.2447, 0.6652],\n",
       "        [0.3333, 0.3333, 0.3333]], dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.5134, 0.8228, 1.6638], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "input = torch.tensor([[1, 2, 3], [1, 2, 3], [3, 3, 3]], dtype=float)\n",
    "display(input)\n",
    "output = softmax(input)\n",
    "display(output)\n",
    "sum(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e24dbb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 900),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(900, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        nn_out = self.linear_relu_stack(x)\n",
    "        return nn.Softmax(dim=1)(nn_out)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        raise RuntimeError(\"Use forward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "357a97db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=900, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=900, out_features=300, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_network = PolicyNetwork(4, 2)\n",
    "policy_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8763d75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c65c4c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57f14cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2641, 0.3076, 0.7351, 0.2398],\n",
       "        [0.2376, 0.7746, 0.4289, 0.3967],\n",
       "        [0.5034, 0.5841, 0.0033, 0.4338],\n",
       "        [0.6472, 0.5605, 0.2108, 0.8040],\n",
       "        [0.0029, 0.7294, 0.5659, 0.0165]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_states = torch.rand(5, 4)\n",
    "mock_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf6c7db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5258, 0.4742],\n",
       "        [0.5204, 0.4796],\n",
       "        [0.5303, 0.4697],\n",
       "        [0.5291, 0.4709],\n",
       "        [0.5199, 0.4801]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_network.forward(mock_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d287d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 900),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(900, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        nn_out = self.linear_relu_stack(x)\n",
    "        return nn_out\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        raise RuntimeError(\"Use forward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a80af706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=900, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=900, out_features=300, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_network = QNetwork(4, 2)\n",
    "q_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e7c0e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0003, -0.0840],\n",
       "        [ 0.0490, -0.0823],\n",
       "        [ 0.0795, -0.0553],\n",
       "        [ 0.0684, -0.0540],\n",
       "        [ 0.0477, -0.1139]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_network.forward(mock_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0aeb9346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(action_f, step_f, env, policy, fail_at_limit=False):\n",
    "    episode_reward = 0\n",
    "    s = env.reset()\n",
    "    for t in itertools.count(start=1):\n",
    "        a = action_f(policy, s)\n",
    "        next_state, reward, failed, info = env.step(a)\n",
    "        episode_reward += reward\n",
    "        assert t <= env._max_episode_steps\n",
    "        limit = t == env._max_episode_steps\n",
    "        if limit and not fail_at_limit:\n",
    "            failed = False\n",
    "        assert fail_at_limit or not (limit and failed)\n",
    "        step_f(s, a, reward, next_state, t, failed, limit)\n",
    "        if failed or limit:\n",
    "            break\n",
    "        s = next_state\n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e867c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, env_state_size, env_action_space_size):\n",
    "        self.policy_network = PolicyNetwork(env_state_size, env_action_space_size)\n",
    "        self.q1_network = QNetwork(env_state_size, env_action_space_size)\n",
    "        self.q2_network = QNetwork(env_state_size, env_action_space_size)\n",
    "        self.q1_target_network = deepcopy(self.q1_network)\n",
    "        self.q2_target_network = deepcopy(self.q2_network)\n",
    "        self.reset_optimizers()\n",
    "\n",
    "    def reset_optimizers(self):\n",
    "        self.policy_optimizer = torch.optim.SGD(self.policy_network.parameters(), lr=POLICY_LR)\n",
    "        self.q1_optimizer = torch.optim.SGD(self.q1_network.parameters(), lr=Q_LR)\n",
    "        self.q2_optimizer = torch.optim.SGD(self.q2_network.parameters(), lr=Q_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2ef4891",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = deque(maxlen=30_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3835dfbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3d9ddd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "856e3ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "oss = env.observation_space.shape\n",
    "if len(oss) != 1:\n",
    "    raise RuntimeError(f'Unknown observation_space.shape: {oss}')\n",
    "os_len = oss[0]\n",
    "policy = Policy(os_len, env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26178da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00290499,  1.4156194 , -0.29424563,  0.20886293,  0.00337281,\n",
       "        0.06665098,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c20fffdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0029,  1.4156, -0.2942,  0.2089,  0.0034,  0.0667,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = torch.tensor(s).reshape((1, -1))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8200ac15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2577, 0.2270, 0.2719, 0.2434]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_output = policy.policy_network.forward(s)\n",
    "policy_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3eb4daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_weights = policy_output.reshape((-1,)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "481df43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b78db85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = random.choices(range(len(action_weights)), weights=action_weights)[0]\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a11637db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action(policy, s):\n",
    "    tensor_s = torch.tensor(s).reshape((1, -1))\n",
    "    action_weights = policy.policy_network.forward(tensor_s).reshape((-1,)).tolist()\n",
    "    action = random.choices(range(len(action_weights)), weights=action_weights)[0]\n",
    "    return action\n",
    "\n",
    "def step(initial_s, a, r, next_s, t, failed, limit):\n",
    "    replay_buffer.append(SARS(initial_s, a, r, next_s, t, failed, limit))\n",
    "    if RENDER:\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b642f7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-79.45620529341322"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_episode(action, step, env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9107af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replay_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e377098",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([SARS(state=array([-0.00662155,  1.3991531 , -0.67071474, -0.52300346,  0.0076796 ,\n",
       "               0.15192688,  0.        ,  0.        ], dtype=float32), action=3, reward=-0.2625017425813201, next_state=array([-0.01317463,  1.3868008 , -0.66114223, -0.5490281 ,  0.01344991,\n",
       "               0.11541824,  0.        ,  0.        ], dtype=float32), t=1, failed=False, limit=False),\n",
       "       SARS(state=array([-0.01317463,  1.3868008 , -0.66114223, -0.5490281 ,  0.01344991,\n",
       "               0.11541824,  0.        ,  0.        ], dtype=float32), action=0, reward=-1.021086704306839, next_state=array([-0.01972799,  1.3738481 , -0.6611601 , -0.5757368 ,  0.01921695,\n",
       "               0.11535144,  0.        ,  0.        ], dtype=float32), t=2, failed=False, limit=False),\n",
       "       SARS(state=array([-0.01972799,  1.3738481 , -0.6611601 , -0.5757368 ,  0.01921695,\n",
       "               0.11535144,  0.        ,  0.        ], dtype=float32), action=1, reward=-1.734157630213075, next_state=array([-0.02634163,  1.3603015 , -0.6687261 , -0.60218126,  0.02649495,\n",
       "               0.14557356,  0.        ,  0.        ], dtype=float32), t=3, failed=False, limit=False),\n",
       "       SARS(state=array([-0.02634163,  1.3603015 , -0.6687261 , -0.60218126,  0.02649495,\n",
       "               0.14557356,  0.        ,  0.        ], dtype=float32), action=2, reward=0.6245689786914397, next_state=array([-0.03312645,  1.3472781 , -0.6851072 , -0.578952  ,  0.0330475 ,\n",
       "               0.1310631 ,  0.        ,  0.        ], dtype=float32), t=4, failed=False, limit=False),\n",
       "       SARS(state=array([-0.03312645,  1.3472781 , -0.6851072 , -0.578952  ,  0.0330475 ,\n",
       "               0.1310631 ,  0.        ,  0.        ], dtype=float32), action=3, reward=-0.1470388338117334, next_state=array([-0.0398345 ,  1.3336629 , -0.67546993, -0.6052364 ,  0.03765995,\n",
       "               0.09225766,  0.        ,  0.        ], dtype=float32), t=5, failed=False, limit=False),\n",
       "       SARS(state=array([-0.0398345 ,  1.3336629 , -0.67546993, -0.6052364 ,  0.03765995,\n",
       "               0.09225766,  0.        ,  0.        ], dtype=float32), action=2, reward=-0.719922637190183, next_state=array([-0.04671898,  1.3200073 , -0.6923155 , -0.6070114 ,  0.04149906,\n",
       "               0.07678908,  0.        ,  0.        ], dtype=float32), t=6, failed=False, limit=False),\n",
       "       SARS(state=array([-0.04671898,  1.3200073 , -0.6923155 , -0.6070114 ,  0.04149906,\n",
       "               0.07678908,  0.        ,  0.        ], dtype=float32), action=3, reward=0.10844703422228122, next_state=array([-0.05352831,  1.3057586 , -0.68291   , -0.63333815,  0.04344606,\n",
       "               0.03894376,  0.        ,  0.        ], dtype=float32), t=7, failed=False, limit=False),\n",
       "       SARS(state=array([-0.05352831,  1.3057586 , -0.68291   , -0.63333815,  0.04344606,\n",
       "               0.03894376,  0.        ,  0.        ], dtype=float32), action=0, reward=-0.5748829872497367, next_state=array([-0.06033783,  1.2909098 , -0.68291485, -0.6600047 ,  0.04539325,\n",
       "               0.03894769,  0.        ,  0.        ], dtype=float32), t=8, failed=False, limit=False),\n",
       "       SARS(state=array([-0.06033783,  1.2909098 , -0.68291485, -0.6600047 ,  0.04539325,\n",
       "               0.03894769,  0.        ,  0.        ], dtype=float32), action=3, reward=0.2951179168037765, next_state=array([-6.7071632e-02,  1.2754678e+00, -6.7343134e-01, -6.8630767e-01,\n",
       "               4.5432989e-02,  7.9494156e-04,  0.0000000e+00,  0.0000000e+00],\n",
       "             dtype=float32), t=9, failed=False, limit=False),\n",
       "       SARS(state=array([-6.7071632e-02,  1.2754678e+00, -6.7343134e-01, -6.8630767e-01,\n",
       "               4.5432989e-02,  7.9494156e-04,  0.0000000e+00,  0.0000000e+00],\n",
       "             dtype=float32), action=3, reward=0.46641316990147286, next_state=array([-0.07373047,  1.2594321 , -0.66400903, -0.7126509 ,  0.04358132,\n",
       "              -0.03703676,  0.        ,  0.        ], dtype=float32), t=10, failed=False, limit=False),\n",
       "       SARS(state=array([-0.07373047,  1.2594321 , -0.66400903, -0.7126509 ,  0.04358132,\n",
       "              -0.03703676,  0.        ,  0.        ], dtype=float32), action=3, reward=0.846423900155487, next_state=array([-0.08029833,  1.2428104 , -0.6526013 , -0.7386211 ,  0.03943533,\n",
       "              -0.08292767,  0.        ,  0.        ], dtype=float32), t=11, failed=False, limit=False),\n",
       "       SARS(state=array([-0.08029833,  1.2428104 , -0.6526013 , -0.7386211 ,  0.03943533,\n",
       "              -0.08292767,  0.        ,  0.        ], dtype=float32), action=2, reward=3.809063278663257, next_state=array([-0.0868742 ,  1.2268625 , -0.6535282 , -0.70869124,  0.03540928,\n",
       "              -0.08052827,  0.        ,  0.        ], dtype=float32), t=12, failed=False, limit=False),\n",
       "       SARS(state=array([-0.0868742 ,  1.2268625 , -0.6535282 , -0.70869124,  0.03540928,\n",
       "              -0.08052827,  0.        ,  0.        ], dtype=float32), action=2, reward=2.7897083189699403, next_state=array([-0.09345951,  1.2113025 , -0.65452385, -0.69147   ,  0.03144826,\n",
       "              -0.07922731,  0.        ,  0.        ], dtype=float32), t=13, failed=False, limit=False),\n",
       "       SARS(state=array([-0.09345951,  1.2113025 , -0.65452385, -0.69147   ,  0.03144826,\n",
       "              -0.07922731,  0.        ,  0.        ], dtype=float32), action=1, reward=-0.8836270007615201, next_state=array([-0.10011987,  1.1951352 , -0.66394836, -0.718505  ,  0.02938236,\n",
       "              -0.04132148,  0.        ,  0.        ], dtype=float32), t=14, failed=False, limit=False),\n",
       "       SARS(state=array([-0.10011987,  1.1951352 , -0.66394836, -0.718505  ,  0.02938236,\n",
       "              -0.04132148,  0.        ,  0.        ], dtype=float32), action=1, reward=-0.8794425536311781, next_state=array([-0.10684614,  1.178376  , -0.67220724, -0.7448462 ,  0.02896769,\n",
       "              -0.00829452,  0.        ,  0.        ], dtype=float32), t=15, failed=False, limit=False),\n",
       "       SARS(state=array([-0.10684614,  1.178376  , -0.67220724, -0.7448462 ,  0.02896769,\n",
       "              -0.00829452,  0.        ,  0.        ], dtype=float32), action=0, reward=-0.28876685684986114, next_state=array([-0.1135724 ,  1.1610167 , -0.67220694, -0.77151513,  0.02855277,\n",
       "              -0.00829918,  0.        ,  0.        ], dtype=float32), t=16, failed=False, limit=False),\n",
       "       SARS(state=array([-0.1135724 ,  1.1610167 , -0.67220694, -0.77151513,  0.02855277,\n",
       "              -0.00829918,  0.        ,  0.        ], dtype=float32), action=2, reward=1.8136595394394817, next_state=array([-0.12048483,  1.1441321 , -0.6900299 , -0.75040084,  0.02736144,\n",
       "              -0.02382876,  0.        ,  0.        ], dtype=float32), t=17, failed=False, limit=False),\n",
       "       SARS(state=array([-0.12048483,  1.1441321 , -0.6900299 , -0.75040084,  0.02736144,\n",
       "              -0.02382876,  0.        ,  0.        ], dtype=float32), action=0, reward=-0.19695777183798668, next_state=array([-0.12739706,  1.1266474 , -0.6900257 , -0.77707833,  0.026171  ,\n",
       "              -0.02381095,  0.        ,  0.        ], dtype=float32), t=18, failed=False, limit=False),\n",
       "       SARS(state=array([-0.12739706,  1.1266474 , -0.6900257 , -0.77707833,  0.026171  ,\n",
       "              -0.02381095,  0.        ,  0.        ], dtype=float32), action=3, reward=0.7806268940942129, next_state=array([-0.13421622,  1.1085596 , -0.67834514, -0.8038469 ,  0.02264248,\n",
       "              -0.07057709,  0.        ,  0.        ], dtype=float32), t=19, failed=False, limit=False),\n",
       "       SARS(state=array([-0.13421622,  1.1085596 , -0.67834514, -0.8038469 ,  0.02264248,\n",
       "              -0.07057709,  0.        ,  0.        ], dtype=float32), action=3, reward=0.9456551789692196, next_state=array([-0.14095001,  1.0898762 , -0.6676524 , -0.83029294,  0.01697418,\n",
       "              -0.11337583,  0.        ,  0.        ], dtype=float32), t=20, failed=False, limit=False),\n",
       "       SARS(state=array([-0.14095001,  1.0898762 , -0.6676524 , -0.83029294,  0.01697418,\n",
       "              -0.11337583,  0.        ,  0.        ], dtype=float32), action=3, reward=1.2623936594267786, next_state=array([-0.14758578,  1.0705919 , -0.65535915, -0.8570098 ,  0.00884818,\n",
       "              -0.16253468,  0.        ,  0.        ], dtype=float32), t=21, failed=False, limit=False),\n",
       "       SARS(state=array([-0.14758578,  1.0705919 , -0.65535915, -0.8570098 ,  0.00884818,\n",
       "              -0.16253468,  0.        ,  0.        ], dtype=float32), action=2, reward=2.7749922970558147, next_state=array([-1.5422793e-01,  1.0514498e+00, -6.5598470e-01, -8.5073107e-01,\n",
       "               7.1310310e-04, -1.6271625e-01,  0.0000000e+00,  0.0000000e+00],\n",
       "             dtype=float32), t=22, failed=False, limit=False),\n",
       "       SARS(state=array([-1.5422793e-01,  1.0514498e+00, -6.5598470e-01, -8.5073107e-01,\n",
       "               7.1310310e-04, -1.6271625e-01,  0.0000000e+00,  0.0000000e+00],\n",
       "             dtype=float32), action=2, reward=3.9319962358528757, next_state=array([-0.16073409,  1.0329679 , -0.6430206 , -0.82142866, -0.00678881,\n",
       "              -0.15005186,  0.        ,  0.        ], dtype=float32), t=23, failed=False, limit=False),\n",
       "       SARS(state=array([-0.16073409,  1.0329679 , -0.6430206 , -0.82142866, -0.00678881,\n",
       "              -0.15005186,  0.        ,  0.        ], dtype=float32), action=2, reward=3.2698535708704926, next_state=array([-0.16729221,  1.0153596 , -0.6479486 , -0.78264135, -0.01456434,\n",
       "              -0.15552457,  0.        ,  0.        ], dtype=float32), t=24, failed=False, limit=False),\n",
       "       SARS(state=array([-0.16729221,  1.0153596 , -0.6479486 , -0.78264135, -0.01456434,\n",
       "              -0.15552457,  0.        ,  0.        ], dtype=float32), action=2, reward=0.8458567746541064, next_state=array([-0.17384252,  0.9978264 , -0.64716184, -0.7793497 , -0.02233417,\n",
       "              -0.1553967 ,  0.        ,  0.        ], dtype=float32), t=25, failed=False, limit=False),\n",
       "       SARS(state=array([-0.17384252,  0.9978264 , -0.64716184, -0.7793497 , -0.02233417,\n",
       "              -0.1553967 ,  0.        ,  0.        ], dtype=float32), action=2, reward=0.25611204302030616, next_state=array([-0.18055467,  0.9805368 , -0.6625215 , -0.76857746, -0.03091071,\n",
       "              -0.17153071,  0.        ,  0.        ], dtype=float32), t=26, failed=False, limit=False),\n",
       "       SARS(state=array([-0.18055467,  0.9805368 , -0.6625215 , -0.76857746, -0.03091071,\n",
       "              -0.17153071,  0.        ,  0.        ], dtype=float32), action=2, reward=0.801934716547106, next_state=array([-0.18725757,  0.9633486 , -0.66158473, -0.7641288 , -0.0395123 ,\n",
       "              -0.17203198,  0.        ,  0.        ], dtype=float32), t=27, failed=False, limit=False),\n",
       "       SARS(state=array([-0.18725757,  0.9633486 , -0.66158473, -0.7641288 , -0.0395123 ,\n",
       "              -0.17203198,  0.        ,  0.        ], dtype=float32), action=0, reward=-1.2784648491492305, next_state=array([-0.19396058,  0.94556135, -0.66158503, -0.79080164, -0.04811386,\n",
       "              -0.1720312 ,  0.        ,  0.        ], dtype=float32), t=28, failed=False, limit=False),\n",
       "       SARS(state=array([-0.19396058,  0.94556135, -0.66158503, -0.79080164, -0.04811386,\n",
       "              -0.1720312 ,  0.        ,  0.        ], dtype=float32), action=3, reward=-0.9357304793084882, next_state=array([-0.20058914,  0.9271634 , -0.652259  , -0.8180632 , -0.05859168,\n",
       "              -0.20955686,  0.        ,  0.        ], dtype=float32), t=29, failed=False, limit=False),\n",
       "       SARS(state=array([-0.20058914,  0.9271634 , -0.652259  , -0.8180632 , -0.05859168,\n",
       "              -0.20955686,  0.        ,  0.        ], dtype=float32), action=2, reward=2.7366883545621876, next_state=array([-0.20715423,  0.90937084, -0.64599055, -0.7912255 , -0.06899475,\n",
       "              -0.20806141,  0.        ,  0.        ], dtype=float32), t=30, failed=False, limit=False),\n",
       "       SARS(state=array([-0.20715423,  0.90937084, -0.64599055, -0.7912255 , -0.06899475,\n",
       "              -0.20806141,  0.        ,  0.        ], dtype=float32), action=0, reward=-1.4789962240223815, next_state=array([-0.21371928,  0.89097977, -0.64599144, -0.8179012 , -0.07939775,\n",
       "              -0.20806003,  0.        ,  0.        ], dtype=float32), t=31, failed=False, limit=False),\n",
       "       SARS(state=array([-0.21371928,  0.89097977, -0.64599144, -0.8179012 , -0.07939775,\n",
       "              -0.20806003,  0.        ,  0.        ], dtype=float32), action=1, reward=-1.9107457605821299, next_state=array([-0.22036663,  0.8719887 , -0.6563141 , -0.8445166 , -0.08773838,\n",
       "              -0.16681254,  0.        ,  0.        ], dtype=float32), t=32, failed=False, limit=False),\n",
       "       SARS(state=array([-0.22036663,  0.8719887 , -0.6563141 , -0.8445166 , -0.08773838,\n",
       "              -0.16681254,  0.        ,  0.        ], dtype=float32), action=2, reward=3.3612833667192605, next_state=array([-0.22679672,  0.85342544, -0.6353269 , -0.82550937, -0.09535649,\n",
       "              -0.15236235,  0.        ,  0.        ], dtype=float32), t=33, failed=False, limit=False),\n",
       "       SARS(state=array([-0.22679672,  0.85342544, -0.6353269 , -0.82550937, -0.09535649,\n",
       "              -0.15236235,  0.        ,  0.        ], dtype=float32), action=1, reward=-1.6129315552468586, next_state=array([-0.2333026 ,  0.834262  , -0.6448106 , -0.8520876 , -0.10108083,\n",
       "              -0.11448674,  0.        ,  0.        ], dtype=float32), t=34, failed=False, limit=False),\n",
       "       SARS(state=array([-0.2333026 ,  0.834262  , -0.6448106 , -0.8520876 , -0.10108083,\n",
       "              -0.11448674,  0.        ,  0.        ], dtype=float32), action=0, reward=-0.9908267252213534, next_state=array([-0.23980837,  0.8144989 , -0.64481103, -0.87875706, -0.10680516,\n",
       "              -0.11448647,  0.        ,  0.        ], dtype=float32), t=35, failed=False, limit=False),\n",
       "       SARS(state=array([-0.23980837,  0.8144989 , -0.64481103, -0.87875706, -0.10680516,\n",
       "              -0.11448647,  0.        ,  0.        ], dtype=float32), action=3, reward=-0.6873788188047729, next_state=array([-0.24623275,  0.79412043, -0.63458335, -0.9062987 , -0.11460129,\n",
       "              -0.1559227 ,  0.        ,  0.        ], dtype=float32), t=36, failed=False, limit=False),\n",
       "       SARS(state=array([-0.24623275,  0.79412043, -0.63458335, -0.9062987 , -0.11460129,\n",
       "              -0.1559227 ,  0.        ,  0.        ], dtype=float32), action=2, reward=0.8275425725682737, next_state=array([-0.25275245,  0.7739552 , -0.6434034 , -0.89691085, -0.12312336,\n",
       "              -0.17044112,  0.        ,  0.        ], dtype=float32), t=37, failed=False, limit=False),\n",
       "       SARS(state=array([-0.25275245,  0.7739552 , -0.6434034 , -0.89691085, -0.12312336,\n",
       "              -0.17044112,  0.        ,  0.        ], dtype=float32), action=3, reward=-1.0218603972331823, next_state=array([-0.25920194,  0.75318116, -0.6346028 , -0.92418116, -0.1334224 ,\n",
       "              -0.20598093,  0.        ,  0.        ], dtype=float32), t=38, failed=False, limit=False),\n",
       "       SARS(state=array([-0.25920194,  0.75318116, -0.6346028 , -0.92418116, -0.1334224 ,\n",
       "              -0.20598093,  0.        ,  0.        ], dtype=float32), action=1, reward=-1.8233550090543804, next_state=array([-0.2657484 ,  0.73182607, -0.64678085, -0.9498396 , -0.14125346,\n",
       "              -0.15662062,  0.        ,  0.        ], dtype=float32), t=39, failed=False, limit=False),\n",
       "       SARS(state=array([-0.2657484 ,  0.73182607, -0.64678085, -0.9498396 , -0.14125346,\n",
       "              -0.15662062,  0.        ,  0.        ], dtype=float32), action=2, reward=2.5856699524999103, next_state=array([-0.2721468 ,  0.7107638 , -0.6322828 , -0.9368352 , -0.14879513,\n",
       "              -0.15083352,  0.        ,  0.        ], dtype=float32), t=40, failed=False, limit=False),\n",
       "       SARS(state=array([-0.2721468 ,  0.7107638 , -0.6322828 , -0.9368352 , -0.14879513,\n",
       "              -0.15083352,  0.        ,  0.        ], dtype=float32), action=0, reward=-1.1931639250998103, next_state=array([-0.2785452 ,  0.6891024 , -0.6322838 , -0.96350664, -0.15633677,\n",
       "              -0.15083306,  0.        ,  0.        ], dtype=float32), t=41, failed=False, limit=False),\n",
       "       SARS(state=array([-0.2785452 ,  0.6891024 , -0.6322838 , -0.96350664, -0.15633677,\n",
       "              -0.15083306,  0.        ,  0.        ], dtype=float32), action=2, reward=3.3788681237117375, next_state=array([-0.28494033,  0.66818947, -0.6313427 , -0.93034995, -0.16451234,\n",
       "              -0.16351137,  0.        ,  0.        ], dtype=float32), t=42, failed=False, limit=False),\n",
       "       SARS(state=array([-0.28494033,  0.66818947, -0.6313427 , -0.93034995, -0.16451234,\n",
       "              -0.16351137,  0.        ,  0.        ], dtype=float32), action=2, reward=3.913443896840346, next_state=array([-0.29127437,  0.64813143, -0.62478197, -0.8924458 , -0.17314464,\n",
       "              -0.17264614,  0.        ,  0.        ], dtype=float32), t=43, failed=False, limit=False),\n",
       "       SARS(state=array([-0.29127437,  0.64813143, -0.62478197, -0.8924458 , -0.17314464,\n",
       "              -0.17264614,  0.        ,  0.        ], dtype=float32), action=1, reward=-1.7683315120544012, next_state=array([-0.29768792,  0.62748986, -0.6347695 , -0.91818506, -0.17974626,\n",
       "              -0.13203245,  0.        ,  0.        ], dtype=float32), t=44, failed=False, limit=False),\n",
       "       SARS(state=array([-0.29768792,  0.62748986, -0.6347695 , -0.91818506, -0.17974626,\n",
       "              -0.13203245,  0.        ,  0.        ], dtype=float32), action=2, reward=1.875066746116073, next_state=array([-0.30415058,  0.6072964 , -0.6388899 , -0.89839953, -0.1871499 ,\n",
       "              -0.14807294,  0.        ,  0.        ], dtype=float32), t=45, failed=False, limit=False),\n",
       "       SARS(state=array([-0.30415058,  0.6072964 , -0.6388899 , -0.89839953, -0.1871499 ,\n",
       "              -0.14807294,  0.        ,  0.        ], dtype=float32), action=3, reward=-1.0924600033918626, next_state=array([-0.31052226,  0.58648133, -0.627488  , -0.9263666 , -0.19688834,\n",
       "              -0.19476901,  0.        ,  0.        ], dtype=float32), t=46, failed=False, limit=False),\n",
       "       SARS(state=array([-0.31052226,  0.58648133, -0.627488  , -0.9263666 , -0.19688834,\n",
       "              -0.19476901,  0.        ,  0.        ], dtype=float32), action=3, reward=-1.4646082571062198, next_state=array([-0.31681752,  0.5650338 , -0.61784774, -0.9548201 , -0.20865215,\n",
       "              -0.23527631,  0.        ,  0.        ], dtype=float32), t=47, failed=False, limit=False),\n",
       "       SARS(state=array([-0.31681752,  0.5650338 , -0.61784774, -0.9548201 , -0.20865215,\n",
       "              -0.23527631,  0.        ,  0.        ], dtype=float32), action=0, reward=-1.8312922068573698, next_state=array([-0.3231125 ,  0.54298806, -0.61785114, -0.9814981 , -0.22041586,\n",
       "              -0.23527405,  0.        ,  0.        ], dtype=float32), t=48, failed=False, limit=False),\n",
       "       SARS(state=array([-0.3231125 ,  0.54298806, -0.61785114, -0.9814981 , -0.22041586,\n",
       "              -0.23527405,  0.        ,  0.        ], dtype=float32), action=2, reward=2.895615722813756, next_state=array([-0.329388  ,  0.5217006 , -0.61512405, -0.9480187 , -0.23300856,\n",
       "              -0.25185412,  0.        ,  0.        ], dtype=float32), t=49, failed=False, limit=False),\n",
       "       SARS(state=array([-0.329388  ,  0.5217006 , -0.61512405, -0.9480187 , -0.23300856,\n",
       "              -0.25185412,  0.        ,  0.        ], dtype=float32), action=2, reward=0.9327221396164134, next_state=array([-0.33554387,  0.5005237 , -0.6032222 , -0.9432033 , -0.24556887,\n",
       "              -0.2512062 ,  0.        ,  0.        ], dtype=float32), t=50, failed=False, limit=False),\n",
       "       SARS(state=array([-0.33554387,  0.5005237 , -0.6032222 , -0.9432033 , -0.24556887,\n",
       "              -0.2512062 ,  0.        ,  0.        ], dtype=float32), action=2, reward=2.9320682523655934, next_state=array([-0.34143442,  0.47979534, -0.5771283 , -0.92329824, -0.25772575,\n",
       "              -0.24313731,  0.        ,  0.        ], dtype=float32), t=51, failed=False, limit=False),\n",
       "       SARS(state=array([-0.34143442,  0.47979534, -0.5771283 , -0.92329824, -0.25772575,\n",
       "              -0.24313731,  0.        ,  0.        ], dtype=float32), action=2, reward=3.2352582735319233, next_state=array([-0.34711504,  0.4596946 , -0.55614454, -0.89550877, -0.2699091 ,\n",
       "              -0.24366722,  0.        ,  0.        ], dtype=float32), t=52, failed=False, limit=False),\n",
       "       SARS(state=array([-0.34711504,  0.4596946 , -0.55614454, -0.89550877, -0.2699091 ,\n",
       "              -0.24366722,  0.        ,  0.        ], dtype=float32), action=1, reward=-2.41682307872142, next_state=array([-0.35288772,  0.43903586, -0.5677965 , -0.9199434 , -0.27961814,\n",
       "              -0.1941807 ,  0.        ,  0.        ], dtype=float32), t=53, failed=False, limit=False),\n",
       "       SARS(state=array([-0.35288772,  0.43903586, -0.5677965 , -0.9199434 , -0.27961814,\n",
       "              -0.1941807 ,  0.        ,  0.        ], dtype=float32), action=0, reward=-1.9834216688195738, next_state=array([-0.35866007,  0.4177783 , -0.56779975, -0.94661754, -0.28932709,\n",
       "              -0.1941795 ,  0.        ,  0.        ], dtype=float32), t=54, failed=False, limit=False),\n",
       "       SARS(state=array([-0.35866007,  0.4177783 , -0.56779975, -0.94661754, -0.28932709,\n",
       "              -0.1941795 ,  0.        ,  0.        ], dtype=float32), action=0, reward=-2.016748161903763, next_state=array([-0.36443228,  0.39592195, -0.5678031 , -0.9732918 , -0.299036  ,\n",
       "              -0.19417827,  0.        ,  0.        ], dtype=float32), t=55, failed=False, limit=False),\n",
       "       SARS(state=array([-0.36443228,  0.39592195, -0.5678031 , -0.9732918 , -0.299036  ,\n",
       "              -0.19417827,  0.        ,  0.        ], dtype=float32), action=0, reward=-2.0576562250141706, next_state=array([-0.3702042 ,  0.3734668 , -0.5678065 , -0.999966  , -0.30874488,\n",
       "              -0.194177  ,  0.        ,  0.        ], dtype=float32), t=56, failed=False, limit=False),\n",
       "       SARS(state=array([-0.3702042 ,  0.3734668 , -0.5678065 , -0.999966  , -0.30874488,\n",
       "              -0.194177  ,  0.        ,  0.        ], dtype=float32), action=0, reward=-2.1072591214288536, next_state=array([-0.37597585,  0.35041288, -0.56781   , -1.0266403 , -0.31845367,\n",
       "              -0.19417576,  0.        ,  0.        ], dtype=float32), t=57, failed=False, limit=False),\n",
       "       SARS(state=array([-0.37597585,  0.35041288, -0.56781   , -1.0266403 , -0.31845367,\n",
       "              -0.19417576,  0.        ,  0.        ], dtype=float32), action=2, reward=0.7856417394895174, next_state=array([-0.38154873,  0.32735118, -0.54825515, -1.0269753 , -0.32786176,\n",
       "              -0.188162  ,  0.        ,  0.        ], dtype=float32), t=58, failed=False, limit=False),\n",
       "       SARS(state=array([-0.38154873,  0.32735118, -0.54825515, -1.0269753 , -0.32786176,\n",
       "              -0.188162  ,  0.        ,  0.        ], dtype=float32), action=1, reward=-2.2995060859930434, next_state=array([-0.38720307,  0.30373645, -0.5585929 , -1.0511148 , -0.335002  ,\n",
       "              -0.14280523,  0.        ,  0.        ], dtype=float32), t=59, failed=False, limit=False),\n",
       "       SARS(state=array([-0.38720307,  0.30373645, -0.5585929 , -1.0511148 , -0.335002  ,\n",
       "              -0.14280523,  0.        ,  0.        ], dtype=float32), action=0, reward=-2.078845464174691, next_state=array([-0.39285722,  0.2795223 , -0.55859506, -1.0777857 , -0.34214222,\n",
       "              -0.14280409,  0.        ,  0.        ], dtype=float32), t=60, failed=False, limit=False),\n",
       "       SARS(state=array([-0.39285722,  0.2795223 , -0.55859506, -1.0777857 , -0.34214222,\n",
       "              -0.14280409,  0.        ,  0.        ], dtype=float32), action=2, reward=3.399749155754887, next_state=array([-0.3982303 ,  0.25587845, -0.5305134 , -1.0524688 , -0.3492691 ,\n",
       "              -0.14253762,  0.        ,  0.        ], dtype=float32), t=61, failed=False, limit=False),\n",
       "       SARS(state=array([-0.3982303 ,  0.25587845, -0.5305134 , -1.0524688 , -0.3492691 ,\n",
       "              -0.14253762,  0.        ,  0.        ], dtype=float32), action=0, reward=-2.3002712452308742, next_state=array([-0.40360323,  0.23163523, -0.53051555, -1.0791395 , -0.35639596,\n",
       "              -0.1425371 ,  0.        ,  0.        ], dtype=float32), t=62, failed=False, limit=False),\n",
       "       SARS(state=array([-0.40360323,  0.23163523, -0.53051555, -1.0791395 , -0.35639596,\n",
       "              -0.1425371 ,  0.        ,  0.        ], dtype=float32), action=2, reward=2.2179931133548054, next_state=array([-0.4088583 ,  0.20791963, -0.5180859 , -1.0558883 , -0.36422613,\n",
       "              -0.1566034 ,  0.        ,  0.        ], dtype=float32), t=63, failed=False, limit=False),\n",
       "       SARS(state=array([-0.4088583 ,  0.20791963, -0.5180859 , -1.0558883 , -0.36422613,\n",
       "              -0.1566034 ,  0.        ,  0.        ], dtype=float32), action=0, reward=-2.613477179402139, next_state=array([-0.4141132 ,  0.18360473, -0.51808864, -1.0825598 , -0.37205628,\n",
       "              -0.15660276,  0.        ,  0.        ], dtype=float32), t=64, failed=False, limit=False),\n",
       "       SARS(state=array([-0.4141132 ,  0.18360473, -0.51808864, -1.0825598 , -0.37205628,\n",
       "              -0.15660276,  0.        ,  0.        ], dtype=float32), action=3, reward=-2.8262427475031857, next_state=array([-0.4192844 ,  0.15863565, -0.50748146, -1.1122793 , -0.3822846 ,\n",
       "              -0.20456693,  0.        ,  0.        ], dtype=float32), t=65, failed=False, limit=False),\n",
       "       SARS(state=array([-0.4192844 ,  0.15863565, -0.50748146, -1.1122793 , -0.3822846 ,\n",
       "              -0.20456693,  0.        ,  0.        ], dtype=float32), action=0, reward=-3.1081765174251927, next_state=array([-0.42445523,  0.13306783, -0.50748634, -1.138954  , -0.3925129 ,\n",
       "              -0.20456545,  0.        ,  0.        ], dtype=float32), t=66, failed=False, limit=False),\n",
       "       SARS(state=array([-0.42445523,  0.13306783, -0.50748634, -1.138954  , -0.3925129 ,\n",
       "              -0.20456545,  0.        ,  0.        ], dtype=float32), action=2, reward=9.312649129923795, next_state=array([-0.42950296,  0.10748871, -0.4950592 , -1.139563  , -0.4029099 ,\n",
       "              -0.2079403 ,  1.        ,  0.        ], dtype=float32), t=67, failed=False, limit=False),\n",
       "       SARS(state=array([-0.42950296,  0.10748871, -0.4950592 , -1.139563  , -0.4029099 ,\n",
       "              -0.2079403 ,  1.        ,  0.        ], dtype=float32), action=0, reward=2.13766859656306, next_state=array([-0.43443236,  0.08218633, -0.50153446, -1.1220052 , -0.39559972,\n",
       "               0.14371787,  1.        ,  0.        ], dtype=float32), t=68, failed=False, limit=False),\n",
       "       SARS(state=array([-0.43443236,  0.08218633, -0.50153446, -1.1220052 , -0.39559972,\n",
       "               0.14371787,  1.        ,  0.        ], dtype=float32), action=0, reward=-100, next_state=array([-0.43789187,  0.05987457, -0.3710753 , -0.6972187 , -0.30030096,\n",
       "               4.261471  ,  1.        ,  0.        ], dtype=float32), t=69, failed=True, limit=False)])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88986d2",
   "metadata": {},
   "source": [
    "# Polyak Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "131ea41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3277,  0.2090,  0.3297,  ...,  0.2051, -0.2188,  0.2909],\n",
       "        [ 0.2528, -0.3106, -0.3053,  ..., -0.2703,  0.2191,  0.3401],\n",
       "        [ 0.0430, -0.0486,  0.1010,  ...,  0.1192, -0.3395,  0.0166],\n",
       "        ...,\n",
       "        [ 0.0794,  0.2144,  0.1901,  ...,  0.0754, -0.0803,  0.3365],\n",
       "        [ 0.1511,  0.1732, -0.2590,  ...,  0.2546, -0.0539, -0.2189],\n",
       "        [-0.3249,  0.1417,  0.0578,  ..., -0.2527, -0.0187,  0.0227]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_parameter_1 = next(policy.policy_network.named_parameters())[1]\n",
    "test_parameter_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52883c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0128, 0.0128, 0.0128,  ..., 0.0128, 0.0128, 0.0128],\n",
       "        [0.0128, 0.0128, 0.0128,  ..., 0.0128, 0.0128, 0.0128],\n",
       "        [0.0128, 0.0128, 0.0128,  ..., 0.0128, 0.0128, 0.0128],\n",
       "        ...,\n",
       "        [0.0128, 0.0128, 0.0128,  ..., 0.0128, 0.0128, 0.0128],\n",
       "        [0.0128, 0.0128, 0.0128,  ..., 0.0128, 0.0128, 0.0128],\n",
       "        [0.0128, 0.0128, 0.0128,  ..., 0.0128, 0.0128, 0.0128]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_parameter_2 = test_parameter_1 * 0 + 0.0128\n",
    "test_parameter_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f54a772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2936,  0.1894,  0.2980,  ...,  0.1859, -0.1956,  0.2631],\n",
       "        [ 0.2288, -0.2783, -0.2735,  ..., -0.2420,  0.1985,  0.3073],\n",
       "        [ 0.0400, -0.0424,  0.0922,  ...,  0.1086, -0.3043,  0.0162],\n",
       "        ...,\n",
       "        [ 0.0727,  0.1942,  0.1724,  ...,  0.0691, -0.0710,  0.3041],\n",
       "        [ 0.1373,  0.1571, -0.2318,  ...,  0.2304, -0.0472, -0.1957],\n",
       "        [-0.2911,  0.1288,  0.0533,  ..., -0.2262, -0.0156,  0.0217]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_parameter_1 * 0.9 + test_parameter_2 * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84ceb135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyak_update(network_to_update, target_network, tau=0.001):\n",
    "    with torch.no_grad():\n",
    "        for to_update, target in zip(network_to_update.parameters(), target_network.parameters()):\n",
    "            to_update *= 1-tau\n",
    "            to_update += target * tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb9c708a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 3.1797e-01, -7.9571e-02,  2.9746e-01,  2.7632e-04,  2.9593e-01],\n",
       "        [ 5.1364e-02, -1.1621e-01,  2.5640e-01, -2.1581e-01, -1.0060e-02],\n",
       "        [-1.1234e-01, -6.9210e-02,  3.7045e-01,  4.1787e-01, -3.0456e-02],\n",
       "        ...,\n",
       "        [ 3.3643e-01,  2.6751e-01, -4.3720e-01, -8.4023e-02,  3.1005e-01],\n",
       "        [-2.2432e-02,  4.0653e-01, -2.9094e-01, -1.0232e-01,  2.1357e-01],\n",
       "        [-1.3168e-01,  4.3242e-01,  2.6597e-01,  9.5947e-02, -1.5247e-01]],\n",
       "       requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2534, -0.1465, -0.2794,  0.2493,  0.3167],\n",
       "        [ 0.4129, -0.2002,  0.4287, -0.2244,  0.2414],\n",
       "        [ 0.3810,  0.3812, -0.1373,  0.3810, -0.4304],\n",
       "        ...,\n",
       "        [ 0.4128, -0.2616,  0.1254,  0.2314,  0.3670],\n",
       "        [ 0.2717, -0.0840,  0.3860, -0.2561,  0.2265],\n",
       "        [-0.2464, -0.0903, -0.3841,  0.2254, -0.3334]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 3.1797e-01, -7.9571e-02,  2.9746e-01,  2.7632e-04,  2.9593e-01],\n",
       "        [ 5.1364e-02, -1.1621e-01,  2.5640e-01, -2.1581e-01, -1.0060e-02],\n",
       "        [-1.1234e-01, -6.9210e-02,  3.7045e-01,  4.1787e-01, -3.0456e-02],\n",
       "        ...,\n",
       "        [ 3.3643e-01,  2.6751e-01, -4.3720e-01, -8.4023e-02,  3.1005e-01],\n",
       "        [-2.2432e-02,  4.0653e-01, -2.9094e-01, -1.0232e-01,  2.1357e-01],\n",
       "        [-1.3168e-01,  4.3242e-01,  2.6597e-01,  9.5947e-02, -1.5247e-01]],\n",
       "       requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1963, -0.1398, -0.2217,  0.2244,  0.3147],\n",
       "        [ 0.3768, -0.1918,  0.4115, -0.2235,  0.2162],\n",
       "        [ 0.3317,  0.3361, -0.0865,  0.3847, -0.3904],\n",
       "        ...,\n",
       "        [ 0.4051, -0.2087,  0.0691,  0.1999,  0.3613],\n",
       "        [ 0.2423, -0.0349,  0.3183, -0.2407,  0.2252],\n",
       "        [-0.2349, -0.0380, -0.3191,  0.2125, -0.3153]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_network_1 = QNetwork(5, 3)\n",
    "test_network_2 = QNetwork(5, 3)\n",
    "display(list(test_network_1.parameters())[0])\n",
    "display(list(test_network_2.parameters())[0])\n",
    "polyak_update(test_network_2, test_network_1, 0.1)\n",
    "display(list(test_network_1.parameters())[0])\n",
    "display(list(test_network_2.parameters())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936a6271",
   "metadata": {},
   "source": [
    "# Log Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d9242c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.01005034, 4.60517019])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4.615220521841592"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.10536052, 2.30258509])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2.4079456086518722"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.22314355, 1.60943791])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.8325814637483102"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.51082562, 0.91629073])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.4271163556401456"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.69314718, 0.69314718])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.3862943611198906"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for p in [0.99, 0.9, 0.8, 0.6, 0.5]:\n",
    "    logs = -np.log([p, 1-p])\n",
    "    display(p, logs, sum(logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e37d9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SARS(state=array([-0.43443236,  0.08218633, -0.50153446, -1.1220052 , -0.39559972,\n",
       "         0.14371787,  1.        ,  0.        ], dtype=float32), action=0, reward=-100, next_state=array([-0.43789187,  0.05987457, -0.3710753 , -0.6972187 , -0.30030096,\n",
       "         4.261471  ,  1.        ,  0.        ], dtype=float32), t=69, failed=True, limit=False),\n",
       " SARS(state=array([-0.2333026 ,  0.834262  , -0.6448106 , -0.8520876 , -0.10108083,\n",
       "        -0.11448674,  0.        ,  0.        ], dtype=float32), action=0, reward=-0.9908267252213534, next_state=array([-0.23980837,  0.8144989 , -0.64481103, -0.87875706, -0.10680516,\n",
       "        -0.11448647,  0.        ,  0.        ], dtype=float32), t=35, failed=False, limit=False),\n",
       " SARS(state=array([-0.14758578,  1.0705919 , -0.65535915, -0.8570098 ,  0.00884818,\n",
       "        -0.16253468,  0.        ,  0.        ], dtype=float32), action=2, reward=2.7749922970558147, next_state=array([-1.5422793e-01,  1.0514498e+00, -6.5598470e-01, -8.5073107e-01,\n",
       "         7.1310310e-04, -1.6271625e-01,  0.0000000e+00,  0.0000000e+00],\n",
       "       dtype=float32), t=22, failed=False, limit=False),\n",
       " SARS(state=array([-0.10011987,  1.1951352 , -0.66394836, -0.718505  ,  0.02938236,\n",
       "        -0.04132148,  0.        ,  0.        ], dtype=float32), action=1, reward=-0.8794425536311781, next_state=array([-0.10684614,  1.178376  , -0.67220724, -0.7448462 ,  0.02896769,\n",
       "        -0.00829452,  0.        ,  0.        ], dtype=float32), t=15, failed=False, limit=False)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(replay_buffer, k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4199269",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "![Psudocode](sac_psudocode.png)\n",
    "\n",
    "Source: https://spinningup.openai.com/en/latest/algorithms/sac.html#pseudocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd447619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_min(q1, q2, states):\n",
    "    \n",
    "    def f(q):\n",
    "        state_values = q.forward(states).detach()\n",
    "        return state_values\n",
    "        \n",
    "    return torch.minimum(*map(f, (q1, q2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "856452ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train/q_loss_1': tensor(150.5596, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " 'train/q_loss_2': tensor(150.1703, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " 'train/policy_loss': tensor(-0.3494, grad_fn=<MulBackward0>)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    stats = {}\n",
    "    # Step 11\n",
    "    training_batch = random.sample(replay_buffer, k=min(len(replay_buffer), 100))\n",
    "    # Prep\n",
    "    states = torch.tensor(np.array([sars.state for sars in training_batch]), requires_grad=False)\n",
    "    actions = torch.tensor(np.array([sars.action for sars in training_batch]), requires_grad=False)\n",
    "    actions_hot = nn.functional.one_hot(actions)\n",
    "    rewards = torch.tensor(np.array([sars.reward for sars in training_batch]), requires_grad=False)\n",
    "    next_states = torch.tensor(np.array([sars.next_state for sars in training_batch]), requires_grad=False)\n",
    "    fails = torch.tensor(np.array([sars.failed for sars in training_batch]), dtype=int, requires_grad=False)\n",
    "    # Step 12\n",
    "    next_action_probs = policy.policy_network.forward(next_states).detach()\n",
    "    assert not next_action_probs.requires_grad\n",
    "    next_states_q_min = q_min(policy.q1_target_network, policy.q2_target_network, next_states)\n",
    "    assert not next_states_q_min.requires_grad\n",
    "    next_actions_q_min = torch.sum(next_states_q_min * next_action_probs, 1)\n",
    "    assert not next_actions_q_min.requires_grad\n",
    "    next_actions_entropy = torch.sum(next_action_probs * torch.log(next_action_probs), 1)\n",
    "    assert not next_actions_entropy.requires_grad\n",
    "    y = rewards + GAMMA * (1-fails) * (next_actions_q_min - ALPHA * next_actions_entropy)\n",
    "    assert not y.requires_grad\n",
    "    # Step 13\n",
    "    for qi, q, opt in ((1, policy.q1_network, policy.q1_optimizer),\n",
    "                       (2, policy.q2_network, policy.q2_optimizer)):\n",
    "        assert not states.requires_grad\n",
    "        assert not actions_hot.requires_grad\n",
    "        q_state_action = torch.sum(q.forward(states) * actions_hot, 1)\n",
    "        assert q_state_action.requires_grad\n",
    "        q_loss = torch.mean((q_state_action - y)**2)\n",
    "        stats[f'train/q_loss_{qi}'] = q_loss\n",
    "        assert q_loss.requires_grad\n",
    "        opt.zero_grad()\n",
    "        q_loss.backward()\n",
    "        opt.step()\n",
    "    # Step 14\n",
    "    action_probs = policy.policy_network.forward(states)\n",
    "    assert action_probs.requires_grad\n",
    "    states_q_min = q_min(policy.q1_network, policy.q2_network, states)\n",
    "    assert not states_q_min.requires_grad\n",
    "    actions_q_min = torch.sum(states_q_min * action_probs, 1)\n",
    "    assert actions_q_min.requires_grad\n",
    "    actions_entropy = torch.sum(action_probs * torch.log(action_probs), 1)\n",
    "    assert actions_entropy.requires_grad\n",
    "    policy_loss = -1 * torch.mean(actions_q_min - ALPHA * actions_entropy)\n",
    "    stats['train/policy_loss'] = policy_loss\n",
    "    policy.policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy.policy_optimizer.step()\n",
    "    # Step 15\n",
    "    polyak_update(policy.q1_target_network, policy.q1_network, tau=TAU)\n",
    "    polyak_update(policy.q2_target_network, policy.q2_network, tau=TAU)\n",
    "    stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a2ff62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(policy, replay_buffer):\n",
    "    stats = {}\n",
    "    # Step 11\n",
    "    training_batch = random.sample(replay_buffer, k=min(len(replay_buffer), 100))\n",
    "    # Prep\n",
    "    states = torch.tensor(np.array([sars.state for sars in training_batch]), requires_grad=False)\n",
    "    actions = torch.tensor(np.array([sars.action for sars in training_batch]), requires_grad=False)\n",
    "    actions_hot = nn.functional.one_hot(actions)\n",
    "    rewards = torch.tensor(np.array([sars.reward for sars in training_batch]), requires_grad=False)\n",
    "    next_states = torch.tensor(np.array([sars.next_state for sars in training_batch]), requires_grad=False)\n",
    "    fails = torch.tensor(np.array([sars.failed for sars in training_batch]), dtype=int, requires_grad=False)\n",
    "    # Step 12\n",
    "    next_action_probs = policy.policy_network.forward(next_states).detach()\n",
    "    assert not next_action_probs.requires_grad\n",
    "    next_states_q_min = q_min(policy.q1_target_network, policy.q2_target_network, next_states)\n",
    "    assert not next_states_q_min.requires_grad\n",
    "    next_actions_q_min = torch.sum(next_states_q_min * next_action_probs, 1)\n",
    "    assert not next_actions_q_min.requires_grad\n",
    "    next_actions_entropy = torch.sum(next_action_probs * torch.log(next_action_probs), 1)\n",
    "    assert not next_actions_entropy.requires_grad\n",
    "    y = rewards + GAMMA * (1-fails) * (next_actions_q_min - ALPHA * next_actions_entropy)\n",
    "    assert not y.requires_grad\n",
    "    # Step 13\n",
    "    for qi, q, opt in ((1, policy.q1_network, policy.q1_optimizer),\n",
    "                       (2, policy.q2_network, policy.q2_optimizer)):\n",
    "        assert not states.requires_grad\n",
    "        assert not actions_hot.requires_grad\n",
    "        try:\n",
    "            # Been getting some unexpected errors on this line, so logging exception details.\n",
    "            q_state_action = torch.sum(q.forward(states) * actions_hot, 1)\n",
    "        except RuntimeError:\n",
    "            display(states.shape)\n",
    "            display(actions_hot.shape)\n",
    "        assert q_state_action.requires_grad\n",
    "        q_loss = torch.mean((q_state_action - y)**2)\n",
    "        stats[f'train/q_loss_{qi}'] = q_loss\n",
    "        assert q_loss.requires_grad\n",
    "        opt.zero_grad()\n",
    "        q_loss.backward()\n",
    "        opt.step()\n",
    "    # Step 14\n",
    "    action_probs = policy.policy_network.forward(states)\n",
    "    assert action_probs.requires_grad\n",
    "    states_q_min = q_min(policy.q1_network, policy.q2_network, states)\n",
    "    assert not states_q_min.requires_grad\n",
    "    actions_q_min = torch.sum(states_q_min * action_probs, 1)\n",
    "    assert actions_q_min.requires_grad\n",
    "    actions_entropy = torch.sum(action_probs * torch.log(action_probs), 1)\n",
    "    assert actions_entropy.requires_grad\n",
    "    policy_loss = -1 * torch.mean(actions_q_min - ALPHA * actions_entropy)\n",
    "    stats['train/policy_loss'] = policy_loss\n",
    "    policy.policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy.policy_optimizer.step()\n",
    "    # Step 15\n",
    "    polyak_update(policy.q1_target_network, policy.q1_network, tau=TAU)\n",
    "    polyak_update(policy.q2_target_network, policy.q2_network, tau=TAU)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389e7276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████████████                                                                     | 400/1000 [06:53<14:32,  1.45s/it]"
     ]
    }
   ],
   "source": [
    "tb_writer = SummaryWriter()\n",
    "\n",
    "oss = env.observation_space.shape\n",
    "if len(oss) != 1:\n",
    "    raise RuntimeError(f'Unknown observation_space.shape: {oss}')\n",
    "os_len = oss[0]\n",
    "policy = Policy(os_len, env.action_space.n)\n",
    "\n",
    "replay_buffer = deque(maxlen=30_000)\n",
    "\n",
    "def action(policy, s):\n",
    "    tensor_s = torch.tensor(s).reshape((1, -1))\n",
    "    action_weights = policy.policy_network.forward(tensor_s).reshape((-1,)).tolist()\n",
    "    action = random.choices(range(len(action_weights)), weights=action_weights)[0]\n",
    "    return action\n",
    "\n",
    "def step(initial_s, a, r, next_s, t, failed, limit):\n",
    "    replay_buffer.append(SARS(initial_s, a, r, next_s, t, failed, limit))\n",
    "    if RENDER:\n",
    "        env.render()\n",
    "\n",
    "for episode in tqdm.tqdm(range(1, 1000+1)):\n",
    "    episode_reward = run_episode(action, step, env, policy, fail_at_limit=True)\n",
    "    tb_writer.add_scalar('main/episode_reward', episode_reward, episode)\n",
    "    tb_writer.add_scalar('main/replay_buffer_length', len(replay_buffer), episode)\n",
    "    policy.reset_optimizers()\n",
    "    for training_iteration in range(1, 100+1):\n",
    "        stats = train(policy, replay_buffer)\n",
    "        for stat, value in stats.items():\n",
    "            tb_writer.add_scalar(stat, value, episode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
