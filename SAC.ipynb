{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2601a6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devjac/Code/python/learn-pytorch/.venv/lib/python3.10/site-packages/gym/wrappers/monitoring/video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils.spawn\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import namedtuple, deque\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf90475",
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f9a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.8  # How much we value entropy / exploration, increasing this will increase exploration.\n",
    "GAMMA = 0.99  # How much we value future rewards.\n",
    "TAU = 0.001  # How much q_target is updated when polyak averaging (step 15).\n",
    "LR = 0.001  # Optimizer learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf163657",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89982995",
   "metadata": {},
   "outputs": [],
   "source": [
    "SARS = namedtuple('SARS', 'state, action, reward, next_state, t, failed, limit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2cd7e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652], dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, dtype=torch.float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=0)\n",
    "input = torch.tensor([1, 2, 3], dtype=float)\n",
    "display(input)\n",
    "output = softmax(input)\n",
    "display(output)\n",
    "sum(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10ba09d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [1., 2., 3.],\n",
       "        [3., 3., 3.]], dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0900, 0.2447, 0.6652],\n",
       "        [0.0900, 0.2447, 0.6652],\n",
       "        [0.3333, 0.3333, 0.3333]], dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.5134, 0.8228, 1.6638], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "input = torch.tensor([[1, 2, 3], [1, 2, 3], [3, 3, 3]], dtype=float)\n",
    "display(input)\n",
    "output = softmax(input)\n",
    "display(output)\n",
    "sum(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e24dbb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        nn_out = self.linear_relu_stack(x)\n",
    "        return nn.Softmax(dim=1)(nn_out)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        raise RuntimeError(\"Use forward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "357a97db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=600, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=600, out_features=200, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=200, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_network = PolicyNetwork(4, 2)\n",
    "policy_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8763d75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c65c4c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57f14cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2276, 0.1398, 0.8639, 0.4408],\n",
       "        [0.7641, 0.8586, 0.2373, 0.2574],\n",
       "        [0.7042, 0.0842, 0.7812, 0.9577],\n",
       "        [0.6487, 0.5499, 0.1303, 0.1788],\n",
       "        [0.8241, 0.4027, 0.3977, 0.8717]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_states = torch.rand(5, 4)\n",
    "mock_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf6c7db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5134, 0.4866],\n",
       "        [0.4818, 0.5182],\n",
       "        [0.5123, 0.4877],\n",
       "        [0.4909, 0.5091],\n",
       "        [0.4911, 0.5089]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_network.forward(mock_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d287d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        nn_out = self.linear_relu_stack(x)\n",
    "        return nn_out\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        raise RuntimeError(\"Use forward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a80af706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=600, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=600, out_features=200, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=200, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_network = QNetwork(4, 2)\n",
    "q_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e7c0e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0154, -0.1036],\n",
       "        [-0.0239, -0.0566],\n",
       "        [ 0.0092, -0.1231],\n",
       "        [-0.0412, -0.0572],\n",
       "        [-0.0089, -0.1071]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_network.forward(mock_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0aeb9346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(action_f, step_f, env, policy):\n",
    "    episode_reward = 0\n",
    "    s = env.reset()\n",
    "    for t in itertools.count(start=0):\n",
    "        a = action_f(policy, s)\n",
    "        next_state, reward, failed, info = env.step(a)\n",
    "        episode_reward += reward\n",
    "        assert t <= env._max_episode_steps\n",
    "        limit = t == env._max_episode_steps\n",
    "        if limit:\n",
    "            failed = False\n",
    "        assert not (limit and failed)\n",
    "        step_f(s, a, reward, next_state, t, failed, limit)\n",
    "        if failed or limit:\n",
    "            break\n",
    "        s = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e867c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, env_state_size, env_action_space_size):\n",
    "        self.policy_network = PolicyNetwork(env_state_size, env_action_space_size)\n",
    "        self.q1_network = QNetwork(env_state_size, env_action_space_size)\n",
    "        self.q2_network = QNetwork(env_state_size, env_action_space_size)\n",
    "        self.q1_target_network = deepcopy(self.q1_network)\n",
    "        self.q2_target_network = deepcopy(self.q2_network)\n",
    "        self.policy_optimizer = torch.optim.Adam(self.policy_network.parameters(), lr=LR)\n",
    "        self.q1_optimizer = torch.optim.Adam(self.q1_network.parameters(), lr=LR)\n",
    "        self.q2_optimizer = torch.optim.Adam(self.q2_network.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2ef4891",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = deque(maxlen=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3835dfbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3d9ddd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "856e3ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "oss = env.observation_space.shape\n",
    "if len(oss) != 1:\n",
    "    raise RuntimeError(f'Unknown observation_space.shape: {oss}')\n",
    "os_len = oss[0]\n",
    "policy = Policy(os_len, env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26178da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02057485, -0.01812934,  0.02035434, -0.02117978], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c20fffdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0206, -0.0181,  0.0204, -0.0212]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = torch.tensor(s).reshape((1, -1))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8200ac15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5068, 0.4932]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_output = policy.policy_network.forward(s)\n",
    "policy_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3eb4daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_weights = policy_output.reshape((-1,)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "481df43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b78db85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = random.choices(range(len(action_weights)), weights=action_weights)[0]\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a11637db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action(policy, s):\n",
    "    tensor_s = torch.tensor(s).reshape((1, -1))\n",
    "    action_weights = policy.policy_network.forward(tensor_s).reshape((-1,)).tolist()\n",
    "    action = random.choices(range(len(action_weights)), weights=action_weights)[0]\n",
    "    return action\n",
    "\n",
    "def step(initial_s, a, r, next_s, t, failed, limit):\n",
    "    replay_buffer.append(SARS(initial_s, a, r, next_s, t, failed, limit))\n",
    "    if RENDER:\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b642f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_episode(action, step, env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9107af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replay_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e377098",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([SARS(state=array([-0.03245957, -0.02608964,  0.01833498,  0.0409357 ], dtype=float32), action=0, reward=1.0, next_state=array([-0.03298136, -0.22146964,  0.0191537 ,  0.33934665], dtype=float32), t=0, failed=False, limit=False),\n",
       "       SARS(state=array([-0.03298136, -0.22146964,  0.0191537 ,  0.33934665], dtype=float32), action=1, reward=1.0, next_state=array([-0.03741075, -0.0266254 ,  0.02594063,  0.05276472], dtype=float32), t=1, failed=False, limit=False),\n",
       "       SARS(state=array([-0.03741075, -0.0266254 ,  0.02594063,  0.05276472], dtype=float32), action=0, reward=1.0, next_state=array([-0.03794326, -0.22210951,  0.02699593,  0.35351792], dtype=float32), t=2, failed=False, limit=False),\n",
       "       SARS(state=array([-0.03794326, -0.22210951,  0.02699593,  0.35351792], dtype=float32), action=1, reward=1.0, next_state=array([-0.04238545, -0.02738163,  0.03406629,  0.06946837], dtype=float32), t=3, failed=False, limit=False),\n",
       "       SARS(state=array([-0.04238545, -0.02738163,  0.03406629,  0.06946837], dtype=float32), action=1, reward=1.0, next_state=array([-0.04293308,  0.16723578,  0.03545565, -0.21227482], dtype=float32), t=4, failed=False, limit=False),\n",
       "       SARS(state=array([-0.04293308,  0.16723578,  0.03545565, -0.21227482], dtype=float32), action=0, reward=1.0, next_state=array([-0.03958837, -0.02837469,  0.03121016,  0.09137825], dtype=float32), t=5, failed=False, limit=False),\n",
       "       SARS(state=array([-0.03958837, -0.02837469,  0.03121016,  0.09137825], dtype=float32), action=0, reward=1.0, next_state=array([-0.04015586, -0.22392976,  0.03303772,  0.3937423 ], dtype=float32), t=6, failed=False, limit=False),\n",
       "       SARS(state=array([-0.04015586, -0.22392976,  0.03303772,  0.3937423 ], dtype=float32), action=1, reward=1.0, next_state=array([-0.04463446, -0.02929182,  0.04091257,  0.11165603], dtype=float32), t=7, failed=False, limit=False),\n",
       "       SARS(state=array([-0.04463446, -0.02929182,  0.04091257,  0.11165603], dtype=float32), action=0, reward=1.0, next_state=array([-0.04522029, -0.2249754 ,  0.04314569,  0.41696075], dtype=float32), t=8, failed=False, limit=False),\n",
       "       SARS(state=array([-0.04522029, -0.2249754 ,  0.04314569,  0.41696075], dtype=float32), action=1, reward=1.0, next_state=array([-0.0497198 , -0.03049062,  0.0514849 ,  0.13818595], dtype=float32), t=9, failed=False, limit=False),\n",
       "       SARS(state=array([-0.0497198 , -0.03049062,  0.0514849 ,  0.13818595], dtype=float32), action=0, reward=1.0, next_state=array([-0.05032961, -0.22631073,  0.05424862,  0.44665676], dtype=float32), t=10, failed=False, limit=False),\n",
       "       SARS(state=array([-0.05032961, -0.22631073,  0.05424862,  0.44665676], dtype=float32), action=1, reward=1.0, next_state=array([-0.05485583, -0.03199651,  0.06318176,  0.17155549], dtype=float32), t=11, failed=False, limit=False),\n",
       "       SARS(state=array([-0.05485583, -0.03199651,  0.06318176,  0.17155549], dtype=float32), action=1, reward=1.0, next_state=array([-0.05549576,  0.16216691,  0.06661287, -0.10054543], dtype=float32), t=12, failed=False, limit=False),\n",
       "       SARS(state=array([-0.05549576,  0.16216691,  0.06661287, -0.10054543], dtype=float32), action=1, reward=1.0, next_state=array([-0.05225242,  0.3562741 ,  0.06460196, -0.37149075], dtype=float32), t=13, failed=False, limit=False),\n",
       "       SARS(state=array([-0.05225242,  0.3562741 ,  0.06460196, -0.37149075], dtype=float32), action=1, reward=1.0, next_state=array([-0.04512694,  0.5504216 ,  0.05717214, -0.64312476], dtype=float32), t=14, failed=False, limit=False),\n",
       "       SARS(state=array([-0.04512694,  0.5504216 ,  0.05717214, -0.64312476], dtype=float32), action=1, reward=1.0, next_state=array([-0.03411851,  0.74470204,  0.04430965, -0.9172699 ], dtype=float32), t=15, failed=False, limit=False),\n",
       "       SARS(state=array([-0.03411851,  0.74470204,  0.04430965, -0.9172699 ], dtype=float32), action=0, reward=1.0, next_state=array([-0.01922446,  0.5490099 ,  0.02596425, -0.610997  ], dtype=float32), t=16, failed=False, limit=False),\n",
       "       SARS(state=array([-0.01922446,  0.5490099 ,  0.02596425, -0.610997  ], dtype=float32), action=0, reward=1.0, next_state=array([-0.00824427,  0.35353488,  0.01374431, -0.3102506 ], dtype=float32), t=17, failed=False, limit=False),\n",
       "       SARS(state=array([-0.00824427,  0.35353488,  0.01374431, -0.3102506 ], dtype=float32), action=1, reward=1.0, next_state=array([-0.00117357,  0.54845834,  0.0075393 , -0.5985675 ], dtype=float32), t=18, failed=False, limit=False),\n",
       "       SARS(state=array([-0.00117357,  0.54845834,  0.0075393 , -0.5985675 ], dtype=float32), action=0, reward=1.0, next_state=array([ 0.0097956 ,  0.3532317 , -0.00443205, -0.30351934], dtype=float32), t=19, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.0097956 ,  0.3532317 , -0.00443205, -0.30351934], dtype=float32), action=1, reward=1.0, next_state=array([ 0.01686023,  0.54841655, -0.01050244, -0.5975967 ], dtype=float32), t=20, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.01686023,  0.54841655, -0.01050244, -0.5975967 ], dtype=float32), action=0, reward=1.0, next_state=array([ 0.02782856,  0.35344312, -0.02245437, -0.30824038], dtype=float32), t=21, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.02782856,  0.35344312, -0.02245437, -0.30824038], dtype=float32), action=0, reward=1.0, next_state=array([ 0.03489742,  0.1586482 , -0.02861918, -0.02272268], dtype=float32), t=22, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.03489742,  0.1586482 , -0.02861918, -0.02272268], dtype=float32), action=1, reward=1.0, next_state=array([ 0.03807039,  0.35416862, -0.02907363, -0.32429612], dtype=float32), t=23, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.03807039,  0.35416862, -0.02907363, -0.32429612], dtype=float32), action=1, reward=1.0, next_state=array([ 0.04515376,  0.5496922 , -0.03555956, -0.62600404], dtype=float32), t=24, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.04515376,  0.5496922 , -0.03555956, -0.62600404], dtype=float32), action=0, reward=1.0, next_state=array([ 0.05614761,  0.35508424, -0.04807964, -0.3447289 ], dtype=float32), t=25, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.05614761,  0.35508424, -0.04807964, -0.3447289 ], dtype=float32), action=1, reward=1.0, next_state=array([ 0.06324929,  0.550856  , -0.05497421, -0.65217716], dtype=float32), t=26, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.06324929,  0.550856  , -0.05497421, -0.65217716], dtype=float32), action=1, reward=1.0, next_state=array([ 0.07426641,  0.7466987 , -0.06801776, -0.9616517 ], dtype=float32), t=27, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.07426641,  0.7466987 , -0.06801776, -0.9616517 ], dtype=float32), action=0, reward=1.0, next_state=array([ 0.08920038,  0.55255353, -0.08725079, -0.6910891 ], dtype=float32), t=28, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.08920038,  0.55255353, -0.08725079, -0.6910891 ], dtype=float32), action=0, reward=1.0, next_state=array([ 0.10025145,  0.3587436 , -0.10107257, -0.42709932], dtype=float32), t=29, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.10025145,  0.3587436 , -0.10107257, -0.42709932], dtype=float32), action=1, reward=1.0, next_state=array([ 0.10742633,  0.555141  , -0.10961456, -0.74985665], dtype=float32), t=30, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.10742633,  0.555141  , -0.10961456, -0.74985665], dtype=float32), action=1, reward=1.0, next_state=array([ 0.11852915,  0.7515904 , -0.12461169, -1.0749243 ], dtype=float32), t=31, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.11852915,  0.7515904 , -0.12461169, -1.0749243 ], dtype=float32), action=0, reward=1.0, next_state=array([ 0.13356096,  0.55831546, -0.14611018, -0.8238011 ], dtype=float32), t=32, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.13356096,  0.55831546, -0.14611018, -0.8238011 ], dtype=float32), action=0, reward=1.0, next_state=array([ 0.14472726,  0.36546195, -0.1625862 , -0.58040684], dtype=float32), t=33, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.14472726,  0.36546195, -0.1625862 , -0.58040684], dtype=float32), action=1, reward=1.0, next_state=array([ 0.1520365 ,  0.5624439 , -0.17419434, -0.91957307], dtype=float32), t=34, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.1520365 ,  0.5624439 , -0.17419434, -0.91957307], dtype=float32), action=0, reward=1.0, next_state=array([ 0.16328537,  0.37004977, -0.1925858 , -0.68630385], dtype=float32), t=35, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.16328537,  0.37004977, -0.1925858 , -0.68630385], dtype=float32), action=0, reward=1.0, next_state=array([ 0.17068638,  0.17804871, -0.20631188, -0.45989755], dtype=float32), t=36, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.17068638,  0.17804871, -0.20631188, -0.45989755], dtype=float32), action=0, reward=1.0, next_state=array([ 0.17424735, -0.01365132, -0.21550983, -0.23867184], dtype=float32), t=37, failed=True, limit=False)])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88986d2",
   "metadata": {},
   "source": [
    "# Polyak Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "131ea41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0737, -0.4976,  0.2914,  0.3621],\n",
       "        [ 0.3490,  0.1600,  0.1528, -0.0690],\n",
       "        [ 0.2899,  0.4529,  0.1889, -0.0258],\n",
       "        ...,\n",
       "        [-0.1919, -0.1023, -0.3327,  0.2168],\n",
       "        [ 0.2567,  0.0165, -0.4353, -0.2538],\n",
       "        [ 0.3787,  0.0832,  0.1555,  0.0082]], requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_parameter_1 = next(policy.policy_network.named_parameters())[1]\n",
    "test_parameter_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52883c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0128, 0.0128, 0.0128, 0.0128],\n",
       "        [0.0128, 0.0128, 0.0128, 0.0128],\n",
       "        [0.0128, 0.0128, 0.0128, 0.0128],\n",
       "        ...,\n",
       "        [0.0128, 0.0128, 0.0128, 0.0128],\n",
       "        [0.0128, 0.0128, 0.0128, 0.0128],\n",
       "        [0.0128, 0.0128, 0.0128, 0.0128]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_parameter_2 = test_parameter_1 * 0 + 0.0128\n",
    "test_parameter_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f54a772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0676, -0.4466,  0.2635,  0.3272],\n",
       "        [ 0.3154,  0.1453,  0.1388, -0.0608],\n",
       "        [ 0.2622,  0.4089,  0.1713, -0.0219],\n",
       "        ...,\n",
       "        [-0.1715, -0.0908, -0.2981,  0.1964],\n",
       "        [ 0.2323,  0.0161, -0.3905, -0.2271],\n",
       "        [ 0.3421,  0.0762,  0.1413,  0.0086]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_parameter_1 * 0.9 + test_parameter_2 * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd475898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyak_update(network_to_update, target_network, tau=0.001):\n",
    "    with torch.no_grad():\n",
    "        for to_update, target in zip(network_to_update.parameters(), target_network.parameters()):\n",
    "            to_update *= 1-tau\n",
    "            to_update += target * tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8fb10776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0298,  0.0279, -0.0442,  0.0963, -0.0846],\n",
       "        [ 0.1909,  0.2909, -0.3064, -0.3638, -0.0099],\n",
       "        [ 0.1050,  0.2826,  0.0005,  0.2853, -0.1424],\n",
       "        ...,\n",
       "        [-0.1232, -0.3184,  0.1565,  0.2264, -0.3666],\n",
       "        [ 0.2386, -0.2276,  0.3120,  0.1630, -0.0011],\n",
       "        [ 0.1266, -0.2548, -0.2124, -0.4007,  0.0891]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2674, -0.2624,  0.1772, -0.1483, -0.1299],\n",
       "        [ 0.4193,  0.4128,  0.3540, -0.2280, -0.0863],\n",
       "        [ 0.4243, -0.1306,  0.2992,  0.4241, -0.0326],\n",
       "        ...,\n",
       "        [ 0.0617, -0.3670,  0.1081, -0.1708, -0.4075],\n",
       "        [-0.3419, -0.3378,  0.1305, -0.3625, -0.4252],\n",
       "        [-0.2501, -0.1479, -0.1584, -0.2678, -0.3493]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0298,  0.0279, -0.0442,  0.0963, -0.0846],\n",
       "        [ 0.1909,  0.2909, -0.3064, -0.3638, -0.0099],\n",
       "        [ 0.1050,  0.2826,  0.0005,  0.2853, -0.1424],\n",
       "        ...,\n",
       "        [-0.1232, -0.3184,  0.1565,  0.2264, -0.3666],\n",
       "        [ 0.2386, -0.2276,  0.3120,  0.1630, -0.0011],\n",
       "        [ 0.1266, -0.2548, -0.2124, -0.4007,  0.0891]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2436, -0.2334,  0.1550, -0.1238, -0.1253],\n",
       "        [ 0.3965,  0.4006,  0.2879, -0.2416, -0.0786],\n",
       "        [ 0.3924, -0.0893,  0.2693,  0.4102, -0.0436],\n",
       "        ...,\n",
       "        [ 0.0432, -0.3621,  0.1129, -0.1311, -0.4034],\n",
       "        [-0.2838, -0.3268,  0.1487, -0.3100, -0.3828],\n",
       "        [-0.2125, -0.1586, -0.1638, -0.2811, -0.3055]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_network_1 = QNetwork(5, 3)\n",
    "test_network_2 = QNetwork(5, 3)\n",
    "display(list(test_network_1.parameters())[0])\n",
    "display(list(test_network_2.parameters())[0])\n",
    "polyak_update(test_network_2, test_network_1, 0.1)\n",
    "display(list(test_network_1.parameters())[0])\n",
    "display(list(test_network_2.parameters())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936a6271",
   "metadata": {},
   "source": [
    "# Log Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d9242c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.01005034, 4.60517019])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4.615220521841592"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.10536052, 2.30258509])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2.4079456086518722"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.22314355, 1.60943791])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.8325814637483102"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.51082562, 0.91629073])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.4271163556401456"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.69314718, 0.69314718])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.3862943611198906"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for p in [0.99, 0.9, 0.8, 0.6, 0.5]:\n",
    "    logs = -np.log([p, 1-p])\n",
    "    display(p, logs, sum(logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e37d9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SARS(state=array([ 0.03807039,  0.35416862, -0.02907363, -0.32429612], dtype=float32), action=1, reward=1.0, next_state=array([ 0.04515376,  0.5496922 , -0.03555956, -0.62600404], dtype=float32), t=24, failed=False, limit=False),\n",
       " SARS(state=array([ 0.13356096,  0.55831546, -0.14611018, -0.8238011 ], dtype=float32), action=0, reward=1.0, next_state=array([ 0.14472726,  0.36546195, -0.1625862 , -0.58040684], dtype=float32), t=33, failed=False, limit=False),\n",
       " SARS(state=array([-0.04522029, -0.2249754 ,  0.04314569,  0.41696075], dtype=float32), action=1, reward=1.0, next_state=array([-0.0497198 , -0.03049062,  0.0514849 ,  0.13818595], dtype=float32), t=9, failed=False, limit=False),\n",
       " SARS(state=array([-0.00117357,  0.54845834,  0.0075393 , -0.5985675 ], dtype=float32), action=0, reward=1.0, next_state=array([ 0.0097956 ,  0.3532317 , -0.00443205, -0.30351934], dtype=float32), t=19, failed=False, limit=False)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(replay_buffer, k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4199269",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "![Psudocode](sac_psudocode.png)\n",
    "\n",
    "Source: https://spinningup.openai.com/en/latest/algorithms/sac.html#pseudocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d7477181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_min(q1, q2, states, actions_hot):\n",
    "    \n",
    "    def f(q):\n",
    "        state_values = q.forward(states).detach()\n",
    "        chosen_action_values = torch.sum(state_values * actions_hot, 1)\n",
    "        return chosen_action_values\n",
    "        \n",
    "    return torch.minimum(*map(f, (q1, q2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "856452ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Step 11\n",
    "training_batch = random.sample(replay_buffer, k=min(len(replay_buffer), 100))\n",
    "# Step 12\n",
    "states = torch.tensor(np.array([sars.state for sars in training_batch]), requires_grad=False)\n",
    "actions = torch.tensor(np.array([sars.action for sars in training_batch]), requires_grad=False)\n",
    "actions_hot = nn.functional.one_hot(actions)\n",
    "rewards = torch.tensor(np.array([sars.reward for sars in training_batch]), requires_grad=False)\n",
    "next_states = torch.tensor(np.array([sars.next_state for sars in training_batch]), requires_grad=False)\n",
    "fails = torch.tensor(np.array([sars.failed for sars in training_batch]), dtype=int, requires_grad=False)\n",
    "next_action_probs = policy.policy_network.forward(next_states).detach()\n",
    "\n",
    "def to_action_probs(probs):\n",
    "    action_index = random.choices(range(len(probs)), weights=probs)[0]\n",
    "    a = action_index\n",
    "    p = probs[action_index]\n",
    "    return [a, p]\n",
    "\n",
    "next_actions_with_probs = np.apply_along_axis(to_action_probs, 1, next_action_probs.numpy())\n",
    "# The next action chosen by the policy network\n",
    "next_actions = torch.tensor(next_actions_with_probs[:, 0], dtype=int, requires_grad=False)\n",
    "next_actions_hot = nn.functional.one_hot(next_actions)\n",
    "# The probability of the next action chosen by the policy network\n",
    "next_probs = torch.tensor(next_actions_with_probs[:, 1], requires_grad=False)\n",
    "sampled_next_action_q_min = q_min(\n",
    "    policy.q1_target_network,\n",
    "    policy.q2_target_network,\n",
    "    next_states,\n",
    "    next_actions_hot)\n",
    "sampled_next_action_prob = torch.sum(next_action_probs * next_actions_hot, 1)\n",
    "sampled_next_action_log_prob = torch.log(sampled_next_action_prob)\n",
    "y = rewards + GAMMA * (1-fails) * (sampled_next_action_q_min - ALPHA * sampled_next_action_log_prob)\n",
    "# Step 13\n",
    "for tensor in (states, actions_hot, y):\n",
    "    assert not tensor.requires_grad\n",
    "for q, opt in ((policy.q1_network, policy.q1_optimizer),\n",
    "               (policy.q2_network, policy.q2_optimizer)):\n",
    "    opt.zero_grad()\n",
    "    q_state_action = torch.sum(q.forward(states) * actions_hot, 1)\n",
    "    assert q_state_action.requires_grad\n",
    "    q_loss = torch.mean((q_state_action - y)**2)\n",
    "    q_loss.backward()\n",
    "    opt.step()\n",
    "# Step 14\n",
    "policy.policy_optimizer.zero_grad()\n",
    "action_probs = policy.policy_network.forward(states)\n",
    "assert action_probs.requires_grad\n",
    "sampled_action_probs = torch.tensor(np.apply_along_axis(to_action_probs, 1, action_probs.detach().numpy()))\n",
    "assert not sampled_action_probs.requires_grad\n",
    "sampled_actions = sampled_action_probs[:, 0].long()\n",
    "assert not sampled_actions.requires_grad\n",
    "sampled_actions_hot = nn.functional.one_hot(sampled_actions)\n",
    "assert not sampled_actions_hot.requires_grad\n",
    "sampled_probs = torch.sum(action_probs * sampled_actions_hot, 1)\n",
    "assert sampled_probs.requires_grad\n",
    "sampled_log_probs = torch.log(sampled_probs)\n",
    "assert sampled_log_probs.requires_grad\n",
    "sampled_action_q_min = q_min(policy.q1_network, policy.q2_network, states, sampled_actions.reshape((-1, 1)))\n",
    "assert not sampled_action_q_min.requires_grad\n",
    "policy_loss = torch.mean(sampled_action_q_min - ALPHA * sampled_log_probs)\n",
    "policy_loss.backward()\n",
    "policy.policy_optimizer.step()\n",
    "# Step 15\n",
    "polyak_update(policy.q1_target_network, policy.q1_network, tau=TAU)\n",
    "polyak_update(policy.q2_target_network, policy.q2_network, tau=TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62a32149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(policy, replay_buffer):\n",
    "    # Step 11\n",
    "    training_batch = random.sample(replay_buffer, k=min(len(replay_buffer), 100))\n",
    "    # Step 12\n",
    "    states = torch.tensor(np.array([sars.state for sars in training_batch]), requires_grad=False)\n",
    "    actions = torch.tensor(np.array([sars.action for sars in training_batch]), requires_grad=False)\n",
    "    actions_hot = nn.functional.one_hot(actions)\n",
    "    rewards = torch.tensor(np.array([sars.reward for sars in training_batch]), requires_grad=False)\n",
    "    next_states = torch.tensor(np.array([sars.next_state for sars in training_batch]), requires_grad=False)\n",
    "    fails = torch.tensor(np.array([sars.failed for sars in training_batch]), dtype=int, requires_grad=False)\n",
    "    next_action_probs = policy.policy_network.forward(next_states).detach()\n",
    "\n",
    "    def to_action_probs(probs):\n",
    "        action_index = random.choices(range(len(probs)), weights=probs)[0]\n",
    "        a = action_index\n",
    "        p = probs[action_index]\n",
    "        return [a, p]\n",
    "\n",
    "    next_actions_with_probs = np.apply_along_axis(to_action_probs, 1, next_action_probs.numpy())\n",
    "    # The next action chosen by the policy network\n",
    "    next_actions = torch.tensor(next_actions_with_probs[:, 0], dtype=int, requires_grad=False)\n",
    "    next_actions_hot = nn.functional.one_hot(next_actions)\n",
    "    # The probability of the next action chosen by the policy network\n",
    "    next_probs = torch.tensor(next_actions_with_probs[:, 1], requires_grad=False)\n",
    "    sampled_next_action_q_min = q_min(\n",
    "        policy.q1_target_network,\n",
    "        policy.q2_target_network,\n",
    "        next_states,\n",
    "        next_actions_hot)\n",
    "    sampled_next_action_prob = torch.sum(next_action_probs * next_actions_hot, 1)\n",
    "    sampled_next_action_log_prob = torch.log(sampled_next_action_prob)\n",
    "    y = rewards + GAMMA * (1-fails) * (sampled_next_action_q_min - ALPHA * sampled_next_action_log_prob)\n",
    "    # Step 13\n",
    "    for tensor in (states, actions_hot, y):\n",
    "        assert not tensor.requires_grad\n",
    "    for q, opt in ((policy.q1_network, policy.q1_optimizer),\n",
    "                   (policy.q2_network, policy.q2_optimizer)):\n",
    "        opt.zero_grad()\n",
    "        q_state_action = torch.sum(q.forward(states) * actions_hot, 1)\n",
    "        assert q_state_action.requires_grad\n",
    "        q_loss = torch.mean((q_state_action - y)**2)\n",
    "        q_loss.backward()\n",
    "        opt.step()\n",
    "    # Step 14\n",
    "    policy.policy_optimizer.zero_grad()\n",
    "    action_probs = policy.policy_network.forward(states)\n",
    "    assert action_probs.requires_grad\n",
    "    sampled_action_probs = torch.tensor(np.apply_along_axis(to_action_probs, 1, action_probs.detach().numpy()))\n",
    "    assert not sampled_action_probs.requires_grad\n",
    "    sampled_actions = sampled_action_probs[:, 0].long()\n",
    "    assert not sampled_actions.requires_grad\n",
    "    sampled_actions_hot = nn.functional.one_hot(sampled_actions)\n",
    "    assert not sampled_actions_hot.requires_grad\n",
    "    sampled_probs = torch.sum(action_probs * sampled_actions_hot, 1)\n",
    "    assert sampled_probs.requires_grad\n",
    "    sampled_log_probs = torch.log(sampled_probs)\n",
    "    assert sampled_log_probs.requires_grad\n",
    "    sampled_action_q_min = q_min(policy.q1_network, policy.q2_network, states, sampled_actions.reshape((-1, 1)))\n",
    "    assert not sampled_action_q_min.requires_grad\n",
    "    policy_loss = torch.mean(sampled_action_q_min - ALPHA * sampled_log_probs)\n",
    "    policy_loss.backward()\n",
    "    policy.policy_optimizer.step()\n",
    "    # Step 15\n",
    "    polyak_update(policy.q1_target_network, policy.q1_network, tau=TAU)\n",
    "    polyak_update(policy.q2_target_network, policy.q2_network, tau=TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ed8257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
