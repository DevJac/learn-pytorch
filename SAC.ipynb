{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2601a6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devjac/Code/python/learn-pytorch/.venv/lib/python3.10/site-packages/gym/wrappers/monitoring/video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils.spawn\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import namedtuple, deque\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf90475",
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f9a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.3  # How much we value entropy / exploration, increasing this will increase exploration.\n",
    "GAMMA = 0.99  # How much we value future rewards.\n",
    "TAU = 0.001  # How much q_target is updated when polyak averaging (step 15).\n",
    "POLICY_LR = 0.001  # Policy learning rate.\n",
    "Q_LR = 0.001  # Q learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf163657",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89982995",
   "metadata": {},
   "outputs": [],
   "source": [
    "SARS = namedtuple('SARS', 'state, action, reward, next_state, t, failed, limit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2cd7e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652], dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, dtype=torch.float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=0)\n",
    "input = torch.tensor([1, 2, 3], dtype=float)\n",
    "display(input)\n",
    "output = softmax(input)\n",
    "display(output)\n",
    "sum(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10ba09d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [1., 2., 3.],\n",
       "        [3., 3., 3.]], dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0900, 0.2447, 0.6652],\n",
       "        [0.0900, 0.2447, 0.6652],\n",
       "        [0.3333, 0.3333, 0.3333]], dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.5134, 0.8228, 1.6638], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "input = torch.tensor([[1, 2, 3], [1, 2, 3], [3, 3, 3]], dtype=float)\n",
    "display(input)\n",
    "output = softmax(input)\n",
    "display(output)\n",
    "sum(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e24dbb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        nn_out = self.linear_relu_stack(x)\n",
    "        return nn.Softmax(dim=1)(nn_out)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        raise RuntimeError(\"Use forward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "357a97db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=600, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=600, out_features=200, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=200, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_network = PolicyNetwork(4, 2)\n",
    "policy_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8763d75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c65c4c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57f14cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6190, 0.2480, 0.1420, 0.8914],\n",
       "        [0.1900, 0.7467, 0.5392, 0.6186],\n",
       "        [0.5108, 0.0135, 0.0980, 0.9990],\n",
       "        [0.8812, 0.4234, 0.1902, 0.7100],\n",
       "        [0.4196, 0.7565, 0.3873, 0.6235]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_states = torch.rand(5, 4)\n",
    "mock_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf6c7db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4756, 0.5244],\n",
       "        [0.4884, 0.5116],\n",
       "        [0.4798, 0.5202],\n",
       "        [0.4785, 0.5215],\n",
       "        [0.4795, 0.5205]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_network.forward(mock_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d287d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        nn_out = self.linear_relu_stack(x)\n",
    "        return nn_out\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        raise RuntimeError(\"Use forward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a80af706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=600, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=600, out_features=200, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=200, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_network = QNetwork(4, 2)\n",
    "q_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e7c0e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0454, -0.1411],\n",
       "        [-0.0014, -0.1465],\n",
       "        [ 0.0607, -0.1517],\n",
       "        [ 0.0396, -0.1380],\n",
       "        [-0.0006, -0.1346]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_network.forward(mock_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0aeb9346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(action_f, step_f, env, policy):\n",
    "    episode_reward = 0\n",
    "    s = env.reset()\n",
    "    for t in itertools.count(start=0):\n",
    "        a = action_f(policy, s)\n",
    "        next_state, reward, failed, info = env.step(a)\n",
    "        episode_reward += reward\n",
    "        assert t <= env._max_episode_steps\n",
    "        limit = t == env._max_episode_steps\n",
    "        if limit:\n",
    "            failed = False\n",
    "        assert not (limit and failed)\n",
    "        step_f(s, a, reward, next_state, t, failed, limit)\n",
    "        if failed or limit:\n",
    "            break\n",
    "        s = next_state\n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e867c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, env_state_size, env_action_space_size):\n",
    "        self.policy_network = PolicyNetwork(env_state_size, env_action_space_size)\n",
    "        self.q1_network = QNetwork(env_state_size, env_action_space_size)\n",
    "        self.q2_network = QNetwork(env_state_size, env_action_space_size)\n",
    "        self.q1_target_network = deepcopy(self.q1_network)\n",
    "        self.q2_target_network = deepcopy(self.q2_network)\n",
    "        self.policy_optimizer = torch.optim.SGD(self.policy_network.parameters(), lr=POLICY_LR)\n",
    "        self.q1_optimizer = torch.optim.SGD(self.q1_network.parameters(), lr=Q_LR)\n",
    "        self.q2_optimizer = torch.optim.SGD(self.q2_network.parameters(), lr=Q_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2ef4891",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = deque(maxlen=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3835dfbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3d9ddd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "856e3ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "oss = env.observation_space.shape\n",
    "if len(oss) != 1:\n",
    "    raise RuntimeError(f'Unknown observation_space.shape: {oss}')\n",
    "os_len = oss[0]\n",
    "policy = Policy(os_len, env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26178da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00700782, -0.03484046, -0.00119085,  0.02735358], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c20fffdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0070, -0.0348, -0.0012,  0.0274]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = torch.tensor(s).reshape((1, -1))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8200ac15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5146, 0.4854]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_output = policy.policy_network.forward(s)\n",
    "policy_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3eb4daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_weights = policy_output.reshape((-1,)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "481df43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b78db85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = random.choices(range(len(action_weights)), weights=action_weights)[0]\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a11637db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action(policy, s):\n",
    "    tensor_s = torch.tensor(s).reshape((1, -1))\n",
    "    action_weights = policy.policy_network.forward(tensor_s).reshape((-1,)).tolist()\n",
    "    action = random.choices(range(len(action_weights)), weights=action_weights)[0]\n",
    "    return action\n",
    "\n",
    "def step(initial_s, a, r, next_s, t, failed, limit):\n",
    "    replay_buffer.append(SARS(initial_s, a, r, next_s, t, failed, limit))\n",
    "    if RENDER:\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b642f7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_episode(action, step, env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9107af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replay_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e377098",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([SARS(state=array([ 0.00205541, -0.0131901 , -0.03513844, -0.00672308], dtype=float32), action=1, reward=1.0, next_state=array([ 0.00179161,  0.1824177 , -0.0352729 , -0.31028226], dtype=float32), t=0, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.00179161,  0.1824177 , -0.0352729 , -0.31028226], dtype=float32), action=1, reward=1.0, next_state=array([ 0.00543996,  0.37802398, -0.04147855, -0.61387724], dtype=float32), t=1, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.00543996,  0.37802398, -0.04147855, -0.61387724], dtype=float32), action=1, reward=1.0, next_state=array([ 0.01300044,  0.57370025, -0.0537561 , -0.9193304 ], dtype=float32), t=2, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.01300044,  0.57370025, -0.0537561 , -0.9193304 ], dtype=float32), action=1, reward=1.0, next_state=array([ 0.02447445,  0.769506  , -0.07214271, -1.2284114 ], dtype=float32), t=3, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.02447445,  0.769506  , -0.07214271, -1.2284114 ], dtype=float32), action=1, reward=1.0, next_state=array([ 0.03986457,  0.96547836, -0.09671093, -1.542797  ], dtype=float32), t=4, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.03986457,  0.96547836, -0.09671093, -1.542797  ], dtype=float32), action=1, reward=1.0, next_state=array([ 0.05917414,  1.1616206 , -0.12756687, -1.8640242 ], dtype=float32), t=5, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.05917414,  1.1616206 , -0.12756687, -1.8640242 ], dtype=float32), action=1, reward=1.0, next_state=array([ 0.08240654,  1.3578886 , -0.16484736, -2.1934369 ], dtype=float32), t=6, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.08240654,  1.3578886 , -0.16484736, -2.1934369 ], dtype=float32), action=0, reward=1.0, next_state=array([ 0.10956432,  1.1646986 , -0.2087161 , -1.9558264 ], dtype=float32), t=7, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.10956432,  1.1646986 , -0.2087161 , -1.9558264 ], dtype=float32), action=0, reward=1.0, next_state=array([ 0.13285829,  0.9723149 , -0.24783263, -1.7344315 ], dtype=float32), t=8, failed=True, limit=False)])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88986d2",
   "metadata": {},
   "source": [
    "# Polyak Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "131ea41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0357,  0.4357, -0.1708, -0.4519],\n",
       "        [-0.1181, -0.0746, -0.2929, -0.0853],\n",
       "        [-0.1354, -0.3029, -0.2852, -0.0292],\n",
       "        ...,\n",
       "        [-0.0623, -0.1166, -0.3547,  0.4216],\n",
       "        [-0.3434,  0.3784,  0.1392, -0.0764],\n",
       "        [ 0.3388, -0.0763,  0.0683, -0.2313]], requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_parameter_1 = next(policy.policy_network.named_parameters())[1]\n",
    "test_parameter_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52883c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0128, 0.0128, 0.0128, 0.0128],\n",
       "        [0.0128, 0.0128, 0.0128, 0.0128],\n",
       "        [0.0128, 0.0128, 0.0128, 0.0128],\n",
       "        ...,\n",
       "        [0.0128, 0.0128, 0.0128, 0.0128],\n",
       "        [0.0128, 0.0128, 0.0128, 0.0128],\n",
       "        [0.0128, 0.0128, 0.0128, 0.0128]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_parameter_2 = test_parameter_1 * 0 + 0.0128\n",
    "test_parameter_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f54a772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0334,  0.3934, -0.1524, -0.4054],\n",
       "        [-0.1050, -0.0659, -0.2623, -0.0755],\n",
       "        [-0.1206, -0.2713, -0.2554, -0.0250],\n",
       "        ...,\n",
       "        [-0.0548, -0.1037, -0.3179,  0.3808],\n",
       "        [-0.3078,  0.3418,  0.1265, -0.0675],\n",
       "        [ 0.3062, -0.0674,  0.0627, -0.2069]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_parameter_1 * 0.9 + test_parameter_2 * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05de6456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyak_update(network_to_update, target_network, tau=0.001):\n",
    "    with torch.no_grad():\n",
    "        for to_update, target in zip(network_to_update.parameters(), target_network.parameters()):\n",
    "            to_update *= 1-tau\n",
    "            to_update += target * tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc831c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1086, -0.3811,  0.4121,  0.4447, -0.0613],\n",
       "        [-0.2336, -0.4341,  0.3314, -0.2447,  0.0519],\n",
       "        [-0.2620,  0.0783,  0.3393, -0.1152,  0.1455],\n",
       "        ...,\n",
       "        [-0.3957, -0.2433,  0.4255, -0.0991, -0.3537],\n",
       "        [ 0.0109, -0.2860,  0.1754, -0.2197, -0.1117],\n",
       "        [-0.0800,  0.3955, -0.0851,  0.4172, -0.1765]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3747, -0.3947,  0.4414, -0.0869,  0.3169],\n",
       "        [-0.0810, -0.4107,  0.4362,  0.0486, -0.3011],\n",
       "        [ 0.4085,  0.2738, -0.0793, -0.0953,  0.2122],\n",
       "        ...,\n",
       "        [-0.1386, -0.3274, -0.0685,  0.2992, -0.4003],\n",
       "        [ 0.3086,  0.4255,  0.3919, -0.2712,  0.3654],\n",
       "        [ 0.0938,  0.0681,  0.2434, -0.1826, -0.3493]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1086, -0.3811,  0.4121,  0.4447, -0.0613],\n",
       "        [-0.2336, -0.4341,  0.3314, -0.2447,  0.0519],\n",
       "        [-0.2620,  0.0783,  0.3393, -0.1152,  0.1455],\n",
       "        ...,\n",
       "        [-0.3957, -0.2433,  0.4255, -0.0991, -0.3537],\n",
       "        [ 0.0109, -0.2860,  0.1754, -0.2197, -0.1117],\n",
       "        [-0.0800,  0.3955, -0.0851,  0.4172, -0.1765]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3481, -0.3933,  0.4385, -0.0338,  0.2791],\n",
       "        [-0.0962, -0.4130,  0.4257,  0.0193, -0.2658],\n",
       "        [ 0.3414,  0.2543, -0.0374, -0.0973,  0.2055],\n",
       "        ...,\n",
       "        [-0.1643, -0.3190, -0.0191,  0.2594, -0.3957],\n",
       "        [ 0.2788,  0.3544,  0.3702, -0.2660,  0.3177],\n",
       "        [ 0.0764,  0.1009,  0.2106, -0.1226, -0.3321]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_network_1 = QNetwork(5, 3)\n",
    "test_network_2 = QNetwork(5, 3)\n",
    "display(list(test_network_1.parameters())[0])\n",
    "display(list(test_network_2.parameters())[0])\n",
    "polyak_update(test_network_2, test_network_1, 0.1)\n",
    "display(list(test_network_1.parameters())[0])\n",
    "display(list(test_network_2.parameters())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936a6271",
   "metadata": {},
   "source": [
    "# Log Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d9242c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.01005034, 4.60517019])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4.615220521841592"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.10536052, 2.30258509])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2.4079456086518722"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.22314355, 1.60943791])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.8325814637483102"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.51082562, 0.91629073])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.4271163556401456"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.69314718, 0.69314718])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.3862943611198906"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for p in [0.99, 0.9, 0.8, 0.6, 0.5]:\n",
    "    logs = -np.log([p, 1-p])\n",
    "    display(p, logs, sum(logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e37d9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SARS(state=array([ 0.01300044,  0.57370025, -0.0537561 , -0.9193304 ], dtype=float32), action=1, reward=1.0, next_state=array([ 0.02447445,  0.769506  , -0.07214271, -1.2284114 ], dtype=float32), t=3, failed=False, limit=False),\n",
       " SARS(state=array([ 0.03986457,  0.96547836, -0.09671093, -1.542797  ], dtype=float32), action=1, reward=1.0, next_state=array([ 0.05917414,  1.1616206 , -0.12756687, -1.8640242 ], dtype=float32), t=5, failed=False, limit=False),\n",
       " SARS(state=array([ 0.00543996,  0.37802398, -0.04147855, -0.61387724], dtype=float32), action=1, reward=1.0, next_state=array([ 0.01300044,  0.57370025, -0.0537561 , -0.9193304 ], dtype=float32), t=2, failed=False, limit=False),\n",
       " SARS(state=array([ 0.02447445,  0.769506  , -0.07214271, -1.2284114 ], dtype=float32), action=1, reward=1.0, next_state=array([ 0.03986457,  0.96547836, -0.09671093, -1.542797  ], dtype=float32), t=4, failed=False, limit=False)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(replay_buffer, k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4199269",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "![Psudocode](sac_psudocode.png)\n",
    "\n",
    "Source: https://spinningup.openai.com/en/latest/algorithms/sac.html#pseudocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44a8b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_min(q1, q2, states, actions_hot):\n",
    "    \n",
    "    def f(q):\n",
    "        state_values = q.forward(states).detach()\n",
    "        chosen_action_values = torch.sum(state_values * actions_hot, 1)\n",
    "        return chosen_action_values\n",
    "        \n",
    "    return torch.minimum(*map(f, (q1, q2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3baa3836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_action_probs(probs):\n",
    "    r = torch.rand(probs.shape)\n",
    "    a = torch.max(probs * r, 1).indices\n",
    "    p = torch.sum(nn.functional.one_hot(a) * probs, 1)\n",
    "    return torch.cat((a.reshape(-1, 1), p.reshape(-1, 1)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "856452ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train/q_loss_1': tensor(0.8530, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " 'train/q_loss_2': tensor(1.4261, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " 'train/policy_loss': tensor(-0.1568, grad_fn=<MulBackward0>)}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    stats = {}\n",
    "    # Step 11\n",
    "    training_batch = random.sample(replay_buffer, k=min(len(replay_buffer), 100))\n",
    "    # Step 12\n",
    "    states = torch.tensor(np.array([sars.state for sars in training_batch]), requires_grad=False)\n",
    "    actions = torch.tensor(np.array([sars.action for sars in training_batch]), requires_grad=False)\n",
    "    actions_hot = nn.functional.one_hot(actions)\n",
    "    rewards = torch.tensor(np.array([sars.reward for sars in training_batch]), requires_grad=False)\n",
    "    next_states = torch.tensor(np.array([sars.next_state for sars in training_batch]), requires_grad=False)\n",
    "    fails = torch.tensor(np.array([sars.failed for sars in training_batch]), dtype=int, requires_grad=False)\n",
    "    next_action_probs = policy.policy_network.forward(next_states).detach()\n",
    "    next_actions_with_probs = to_action_probs(next_action_probs)\n",
    "    # The next action chosen by the policy network\n",
    "    next_actions = next_actions_with_probs[:, 0].long()\n",
    "    next_actions_hot = nn.functional.one_hot(next_actions)\n",
    "    # The probability of the next action chosen by the policy network\n",
    "    next_probs = next_actions_with_probs[:, 1]\n",
    "    sampled_next_action_q_min = q_min(\n",
    "        policy.q1_target_network,\n",
    "        policy.q2_target_network,\n",
    "        next_states,\n",
    "        next_actions_hot)\n",
    "    sampled_next_action_prob = torch.sum(next_action_probs * next_actions_hot, 1)\n",
    "    sampled_next_action_log_prob = torch.log(sampled_next_action_prob)\n",
    "    y = rewards + GAMMA * (1-fails) * (sampled_next_action_q_min - ALPHA * sampled_next_action_log_prob)\n",
    "    # Step 13\n",
    "    for tensor in (states, actions_hot, y):\n",
    "        assert not tensor.requires_grad\n",
    "    for qi, q, opt in ((1, policy.q1_network, policy.q1_optimizer),\n",
    "                       (2, policy.q2_network, policy.q2_optimizer)):\n",
    "        opt.zero_grad()\n",
    "        q_state_action = torch.sum(q.forward(states) * actions_hot, 1)\n",
    "        assert q_state_action.requires_grad\n",
    "        q_loss = torch.mean((q_state_action - y)**2)\n",
    "        stats[f'train/q_loss_{qi}'] = q_loss\n",
    "        q_loss.backward()\n",
    "        opt.step()\n",
    "    # Step 14\n",
    "    policy.policy_optimizer.zero_grad()\n",
    "    action_probs = policy.policy_network.forward(states)\n",
    "    assert action_probs.requires_grad\n",
    "    sampled_action_probs = to_action_probs(action_probs)\n",
    "    assert sampled_action_probs.requires_grad\n",
    "    sampled_actions = sampled_action_probs[:, 0].long()\n",
    "    assert not sampled_actions.requires_grad\n",
    "    sampled_actions_hot = nn.functional.one_hot(sampled_actions)\n",
    "    assert not sampled_actions_hot.requires_grad\n",
    "    sampled_probs = torch.sum(action_probs * sampled_actions_hot, 1)\n",
    "    assert sampled_probs.requires_grad\n",
    "    sampled_log_probs = torch.log(sampled_probs)\n",
    "    assert sampled_log_probs.requires_grad\n",
    "    sampled_action_q_min = q_min(policy.q1_network, policy.q2_network, states, sampled_actions.reshape((-1, 1)))\n",
    "    assert not sampled_action_q_min.requires_grad\n",
    "    policy_loss = -1 * torch.mean(sampled_action_q_min - ALPHA * sampled_log_probs)\n",
    "    stats['train/policy_loss'] = policy_loss\n",
    "    policy_loss.backward()\n",
    "    policy.policy_optimizer.step()\n",
    "    # Step 15\n",
    "    polyak_update(policy.q1_target_network, policy.q1_network, tau=TAU)\n",
    "    polyak_update(policy.q2_target_network, policy.q2_network, tau=TAU)\n",
    "    stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5719f7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(policy, replay_buffer):\n",
    "    stats = {}\n",
    "    # Step 11\n",
    "    training_batch = random.sample(replay_buffer, k=min(len(replay_buffer), 100))\n",
    "    # Step 12\n",
    "    states = torch.tensor(np.array([sars.state for sars in training_batch]), requires_grad=False)\n",
    "    actions = torch.tensor(np.array([sars.action for sars in training_batch]), requires_grad=False)\n",
    "    actions_hot = nn.functional.one_hot(actions)\n",
    "    rewards = torch.tensor(np.array([sars.reward for sars in training_batch]), requires_grad=False)\n",
    "    next_states = torch.tensor(np.array([sars.next_state for sars in training_batch]), requires_grad=False)\n",
    "    fails = torch.tensor(np.array([sars.failed for sars in training_batch]), dtype=int, requires_grad=False)\n",
    "    next_action_probs = policy.policy_network.forward(next_states).detach()\n",
    "    next_actions_with_probs = to_action_probs(next_action_probs)\n",
    "    # The next action chosen by the policy network\n",
    "    next_actions = next_actions_with_probs[:, 0].long()\n",
    "    next_actions_hot = nn.functional.one_hot(next_actions)\n",
    "    # The probability of the next action chosen by the policy network\n",
    "    next_probs = next_actions_with_probs[:, 1]\n",
    "    sampled_next_action_q_min = q_min(\n",
    "        policy.q1_target_network,\n",
    "        policy.q2_target_network,\n",
    "        next_states,\n",
    "        next_actions_hot)\n",
    "    sampled_next_action_prob = torch.sum(next_action_probs * next_actions_hot, 1)\n",
    "    sampled_next_action_log_prob = torch.log(sampled_next_action_prob)\n",
    "    y = rewards + GAMMA * (1-fails) * (sampled_next_action_q_min - ALPHA * sampled_next_action_log_prob)\n",
    "    # Step 13\n",
    "    for tensor in (states, actions_hot, y):\n",
    "        assert not tensor.requires_grad\n",
    "    for qi, q, opt in ((1, policy.q1_network, policy.q1_optimizer),\n",
    "                       (2, policy.q2_network, policy.q2_optimizer)):\n",
    "        opt.zero_grad()\n",
    "        q_state_action = torch.sum(q.forward(states) * actions_hot, 1)\n",
    "        assert q_state_action.requires_grad\n",
    "        q_loss = torch.mean((q_state_action - y)**2)\n",
    "        stats[f'train/q_loss_{qi}'] = q_loss\n",
    "        q_loss.backward()\n",
    "        opt.step()\n",
    "    # Step 14\n",
    "    policy.policy_optimizer.zero_grad()\n",
    "    action_probs = policy.policy_network.forward(states)\n",
    "    assert action_probs.requires_grad\n",
    "    sampled_action_probs = to_action_probs(action_probs)\n",
    "    assert sampled_action_probs.requires_grad\n",
    "    sampled_actions = sampled_action_probs[:, 0].long()\n",
    "    assert not sampled_actions.requires_grad\n",
    "    sampled_actions_hot = nn.functional.one_hot(sampled_actions)\n",
    "    assert not sampled_actions_hot.requires_grad\n",
    "    sampled_probs = torch.sum(action_probs * sampled_actions_hot, 1)\n",
    "    assert sampled_probs.requires_grad\n",
    "    sampled_log_probs = torch.log(sampled_probs)\n",
    "    assert sampled_log_probs.requires_grad\n",
    "    sampled_action_q_min = q_min(policy.q1_network, policy.q2_network, states, sampled_actions.reshape((-1, 1)))\n",
    "    assert not sampled_action_q_min.requires_grad\n",
    "    policy_loss = -1 * torch.mean(sampled_action_q_min - ALPHA * sampled_log_probs)\n",
    "    stats['train/policy_loss'] = policy_loss\n",
    "    policy_loss.backward()\n",
    "    policy.policy_optimizer.step()\n",
    "    # Step 15\n",
    "    polyak_update(policy.q1_target_network, policy.q1_network, tau=TAU)\n",
    "    polyak_update(policy.q2_target_network, policy.q2_network, tau=TAU)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0b1167ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▉                                                                                                                    | 8/1000 [00:03<07:20,  2.25it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m tb_writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain/replay_buffer_length\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(replay_buffer), episode)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m training_iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 16\u001b[0m     stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stat, value \u001b[38;5;129;01min\u001b[39;00m stats\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     18\u001b[0m         tb_writer\u001b[38;5;241m.\u001b[39madd_scalar(stat, value, episode)\n",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(policy, replay_buffer)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Step 12\u001b[39;00m\n\u001b[1;32m      6\u001b[0m states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray([sars\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;28;01mfor\u001b[39;00m sars \u001b[38;5;129;01min\u001b[39;00m training_batch]), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msars\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtraining_batch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m actions_hot \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mone_hot(actions)\n\u001b[1;32m      9\u001b[0m rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray([sars\u001b[38;5;241m.\u001b[39mreward \u001b[38;5;28;01mfor\u001b[39;00m sars \u001b[38;5;129;01min\u001b[39;00m training_batch]), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tb_writer = SummaryWriter()\n",
    "\n",
    "oss = env.observation_space.shape\n",
    "if len(oss) != 1:\n",
    "    raise RuntimeError(f'Unknown observation_space.shape: {oss}')\n",
    "os_len = oss[0]\n",
    "policy = Policy(os_len, env.action_space.n)\n",
    "\n",
    "replay_buffer = deque(maxlen=10_000)\n",
    "\n",
    "for episode in tqdm.tqdm(range(1, 1000+1)):\n",
    "    episode_reward = run_episode(action, step, env, policy)\n",
    "    tb_writer.add_scalar('main/episode_reward', episode_reward, episode)\n",
    "    tb_writer.add_scalar('main/replay_buffer_length', len(replay_buffer), episode)\n",
    "    for training_iteration in range(1, 100+1):\n",
    "        stats = train(policy, replay_buffer)\n",
    "        for stat, value in stats.items():\n",
    "            tb_writer.add_scalar(stat, value, episode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
