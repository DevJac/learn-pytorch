{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2601a6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devjac/Code/python/learn-pytorch/.venv/lib/python3.10/site-packages/gym/wrappers/monitoring/video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils.spawn\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import namedtuple, deque\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf90475",
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f9a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.1  # How much we value entropy / exploration, increasing this will increase exploration.\n",
    "GAMMA = 0.99  # How much we value future rewards.\n",
    "TAU = 0.001  # How much q_target is updated when polyak averaging (step 15).\n",
    "POLICY_LR = 0.001  # Policy learning rate.\n",
    "Q_LR = 0.001  # Q learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf163657",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89982995",
   "metadata": {},
   "outputs": [],
   "source": [
    "SARS = namedtuple('SARS', 'state, action, reward, next_state, t, failed, limit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2cd7e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652], dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, dtype=torch.float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=0)\n",
    "input = torch.tensor([1, 2, 3], dtype=float)\n",
    "display(input)\n",
    "output = softmax(input)\n",
    "display(output)\n",
    "sum(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10ba09d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [1., 2., 3.],\n",
       "        [3., 3., 3.]], dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0900, 0.2447, 0.6652],\n",
       "        [0.0900, 0.2447, 0.6652],\n",
       "        [0.3333, 0.3333, 0.3333]], dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.5134, 0.8228, 1.6638], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "input = torch.tensor([[1, 2, 3], [1, 2, 3], [3, 3, 3]], dtype=float)\n",
    "display(input)\n",
    "output = softmax(input)\n",
    "display(output)\n",
    "sum(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e24dbb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        nn_out = self.linear_relu_stack(x)\n",
    "        return nn.Softmax(dim=1)(nn_out)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        raise RuntimeError(\"Use forward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "357a97db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=600, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=600, out_features=200, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=200, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_network = PolicyNetwork(4, 2)\n",
    "policy_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8763d75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c65c4c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57f14cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5955, 0.5279, 0.0574, 0.9862],\n",
       "        [0.7579, 0.5896, 0.7630, 0.6750],\n",
       "        [0.2088, 0.8976, 0.4527, 0.6077],\n",
       "        [0.0098, 0.2516, 0.1567, 0.8262],\n",
       "        [0.2279, 0.5343, 0.1252, 0.8394]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_states = torch.rand(5, 4)\n",
    "mock_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf6c7db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5378, 0.4622],\n",
       "        [0.5489, 0.4511],\n",
       "        [0.5363, 0.4637],\n",
       "        [0.5313, 0.4687],\n",
       "        [0.5301, 0.4699]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_network.forward(mock_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d287d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        nn_out = self.linear_relu_stack(x)\n",
    "        return nn_out\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        raise RuntimeError(\"Use forward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a80af706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=600, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=600, out_features=200, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=200, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_network = QNetwork(4, 2)\n",
    "q_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e7c0e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0071, 0.0846],\n",
       "        [0.0388, 0.0882],\n",
       "        [0.0511, 0.0748],\n",
       "        [0.0644, 0.0569],\n",
       "        [0.0388, 0.0773]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_network.forward(mock_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0aeb9346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(action_f, step_f, env, policy):\n",
    "    episode_reward = 0\n",
    "    s = env.reset()\n",
    "    for t in itertools.count(start=0):\n",
    "        a = action_f(policy, s)\n",
    "        next_state, reward, failed, info = env.step(a)\n",
    "        episode_reward += reward\n",
    "        assert t <= env._max_episode_steps\n",
    "        limit = t == env._max_episode_steps\n",
    "        if limit:\n",
    "            failed = False\n",
    "        assert not (limit and failed)\n",
    "        step_f(s, a, reward, next_state, t, failed, limit)\n",
    "        if failed or limit:\n",
    "            break\n",
    "        s = next_state\n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e867c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, env_state_size, env_action_space_size):\n",
    "        self.policy_network = PolicyNetwork(env_state_size, env_action_space_size)\n",
    "        self.q1_network = QNetwork(env_state_size, env_action_space_size)\n",
    "        self.q2_network = QNetwork(env_state_size, env_action_space_size)\n",
    "        self.q1_target_network = deepcopy(self.q1_network)\n",
    "        self.q2_target_network = deepcopy(self.q2_network)\n",
    "        self.policy_optimizer = torch.optim.SGD(self.policy_network.parameters(), lr=POLICY_LR)\n",
    "        self.q1_optimizer = torch.optim.SGD(self.q1_network.parameters(), lr=Q_LR)\n",
    "        self.q2_optimizer = torch.optim.SGD(self.q2_network.parameters(), lr=Q_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2ef4891",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = deque(maxlen=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3835dfbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3d9ddd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "856e3ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "oss = env.observation_space.shape\n",
    "if len(oss) != 1:\n",
    "    raise RuntimeError(f'Unknown observation_space.shape: {oss}')\n",
    "os_len = oss[0]\n",
    "policy = Policy(os_len, env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26178da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04659428, -0.02196541,  0.01238199, -0.04668314], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c20fffdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0466, -0.0220,  0.0124, -0.0467]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = torch.tensor(s).reshape((1, -1))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8200ac15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4946, 0.5054]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_output = policy.policy_network.forward(s)\n",
    "policy_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3eb4daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_weights = policy_output.reshape((-1,)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "481df43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b78db85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = random.choices(range(len(action_weights)), weights=action_weights)[0]\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a11637db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action(policy, s):\n",
    "    tensor_s = torch.tensor(s).reshape((1, -1))\n",
    "    action_weights = policy.policy_network.forward(tensor_s).reshape((-1,)).tolist()\n",
    "    action = random.choices(range(len(action_weights)), weights=action_weights)[0]\n",
    "    return action\n",
    "\n",
    "def step(initial_s, a, r, next_s, t, failed, limit):\n",
    "    replay_buffer.append(SARS(initial_s, a, r, next_s, t, failed, limit))\n",
    "    if RENDER:\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b642f7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_episode(action, step, env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9107af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replay_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e377098",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([SARS(state=array([ 0.01705702, -0.00205287, -0.03202293,  0.00623765], dtype=float32), action=0, reward=1.0, next_state=array([ 0.01701596, -0.19670129, -0.03189818,  0.28864744], dtype=float32), t=0, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.01701596, -0.19670129, -0.03189818,  0.28864744], dtype=float32), action=1, reward=1.0, next_state=array([ 0.01308193, -0.00113932, -0.02612523, -0.01392275], dtype=float32), t=1, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.01308193, -0.00113932, -0.02612523, -0.01392275], dtype=float32), action=0, reward=1.0, next_state=array([ 0.01305915, -0.19587706, -0.02640369,  0.27040422], dtype=float32), t=2, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.01305915, -0.19587706, -0.02640369,  0.27040422], dtype=float32), action=0, reward=1.0, next_state=array([ 0.00914161, -0.39061245, -0.0209956 ,  0.55464375], dtype=float32), t=3, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.00914161, -0.39061245, -0.0209956 ,  0.55464375], dtype=float32), action=0, reward=1.0, next_state=array([ 0.00132936, -0.5854334 , -0.00990273,  0.8406385 ], dtype=float32), t=4, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.00132936, -0.5854334 , -0.00990273,  0.8406385 ], dtype=float32), action=0, reward=1.0, next_state=array([-0.01037931, -0.7804188 ,  0.00691004,  1.1301908 ], dtype=float32), t=5, failed=False, limit=False),\n",
       "       SARS(state=array([-0.01037931, -0.7804188 ,  0.00691004,  1.1301908 ], dtype=float32), action=1, reward=1.0, next_state=array([-0.02598769, -0.585388  ,  0.02951386,  0.83968323], dtype=float32), t=6, failed=False, limit=False),\n",
       "       SARS(state=array([-0.02598769, -0.585388  ,  0.02951386,  0.83968323], dtype=float32), action=1, reward=1.0, next_state=array([-0.03769545, -0.39068118,  0.04630752,  0.556426  ], dtype=float32), t=7, failed=False, limit=False),\n",
       "       SARS(state=array([-0.03769545, -0.39068118,  0.04630752,  0.556426  ], dtype=float32), action=0, reward=1.0, next_state=array([-0.04550907, -0.5864216 ,  0.05743604,  0.86333144], dtype=float32), t=8, failed=False, limit=False),\n",
       "       SARS(state=array([-0.04550907, -0.5864216 ,  0.05743604,  0.86333144], dtype=float32), action=1, reward=1.0, next_state=array([-0.0572375 , -0.39212668,  0.07470267,  0.5892466 ], dtype=float32), t=9, failed=False, limit=False),\n",
       "       SARS(state=array([-0.0572375 , -0.39212668,  0.07470267,  0.5892466 ], dtype=float32), action=1, reward=1.0, next_state=array([-0.06508004, -0.19812588,  0.08648761,  0.3209991 ], dtype=float32), t=10, failed=False, limit=False),\n",
       "       SARS(state=array([-0.06508004, -0.19812588,  0.08648761,  0.3209991 ], dtype=float32), action=1, reward=1.0, next_state=array([-0.06904256, -0.00433524,  0.09290759,  0.05679529], dtype=float32), t=11, failed=False, limit=False),\n",
       "       SARS(state=array([-0.06904256, -0.00433524,  0.09290759,  0.05679529], dtype=float32), action=1, reward=1.0, next_state=array([-0.06912926,  0.18934025,  0.09404349, -0.20518947], dtype=float32), t=12, failed=False, limit=False),\n",
       "       SARS(state=array([-0.06912926,  0.18934025,  0.09404349, -0.20518947], dtype=float32), action=1, reward=1.0, next_state=array([-0.06534246,  0.3830003 ,  0.0899397 , -0.4667879 ], dtype=float32), t=13, failed=False, limit=False),\n",
       "       SARS(state=array([-0.06534246,  0.3830003 ,  0.0899397 , -0.4667879 ], dtype=float32), action=0, reward=1.0, next_state=array([-0.05768245,  0.1867304 ,  0.08060394, -0.14716631], dtype=float32), t=14, failed=False, limit=False),\n",
       "       SARS(state=array([-0.05768245,  0.1867304 ,  0.08060394, -0.14716631], dtype=float32), action=0, reward=1.0, next_state=array([-0.05394784, -0.00944777,  0.07766062,  0.16981743], dtype=float32), t=15, failed=False, limit=False),\n",
       "       SARS(state=array([-0.05394784, -0.00944777,  0.07766062,  0.16981743], dtype=float32), action=1, reward=1.0, next_state=array([-0.0541368 ,  0.18448167,  0.08105697, -0.09739066], dtype=float32), t=16, failed=False, limit=False),\n",
       "       SARS(state=array([-0.0541368 ,  0.18448167,  0.08105697, -0.09739066], dtype=float32), action=1, reward=1.0, next_state=array([-0.05044716,  0.37835398,  0.07910915, -0.36343965], dtype=float32), t=17, failed=False, limit=False),\n",
       "       SARS(state=array([-0.05044716,  0.37835398,  0.07910915, -0.36343965], dtype=float32), action=0, reward=1.0, next_state=array([-0.04288008,  0.18220203,  0.07184036, -0.04689809], dtype=float32), t=18, failed=False, limit=False),\n",
       "       SARS(state=array([-0.04288008,  0.18220203,  0.07184036, -0.04689809], dtype=float32), action=0, reward=1.0, next_state=array([-0.03923604, -0.01387261,  0.0709024 ,  0.26755813], dtype=float32), t=19, failed=False, limit=False),\n",
       "       SARS(state=array([-0.03923604, -0.01387261,  0.0709024 ,  0.26755813], dtype=float32), action=1, reward=1.0, next_state=array([-0.03951349,  0.1801696 ,  0.07625356, -0.00194602], dtype=float32), t=20, failed=False, limit=False),\n",
       "       SARS(state=array([-0.03951349,  0.1801696 ,  0.07625356, -0.00194602], dtype=float32), action=1, reward=1.0, next_state=array([-0.0359101 ,  0.37411985,  0.07621464, -0.26962915], dtype=float32), t=21, failed=False, limit=False),\n",
       "       SARS(state=array([-0.0359101 ,  0.37411985,  0.07621464, -0.26962915], dtype=float32), action=0, reward=1.0, next_state=array([-0.02842771,  0.17799768,  0.07082205,  0.0460855 ], dtype=float32), t=22, failed=False, limit=False),\n",
       "       SARS(state=array([-0.02842771,  0.17799768,  0.07082205,  0.0460855 ], dtype=float32), action=0, reward=1.0, next_state=array([-0.02486775, -0.01806459,  0.07174376,  0.36024594], dtype=float32), t=23, failed=False, limit=False),\n",
       "       SARS(state=array([-0.02486775, -0.01806459,  0.07174376,  0.36024594], dtype=float32), action=0, reward=1.0, next_state=array([-0.02522904, -0.21412914,  0.07894868,  0.6746608 ], dtype=float32), t=24, failed=False, limit=False),\n",
       "       SARS(state=array([-0.02522904, -0.21412914,  0.07894868,  0.6746608 ], dtype=float32), action=1, reward=1.0, next_state=array([-0.02951163, -0.02018797,  0.0924419 ,  0.40784198], dtype=float32), t=25, failed=False, limit=False),\n",
       "       SARS(state=array([-0.02951163, -0.02018797,  0.0924419 ,  0.40784198], dtype=float32), action=0, reward=1.0, next_state=array([-0.02991539, -0.21649076,  0.10059874,  0.72817814], dtype=float32), t=26, failed=False, limit=False),\n",
       "       SARS(state=array([-0.02991539, -0.21649076,  0.10059874,  0.72817814], dtype=float32), action=1, reward=1.0, next_state=array([-0.0342452 , -0.02289275,  0.11516231,  0.46877548], dtype=float32), t=27, failed=False, limit=False),\n",
       "       SARS(state=array([-0.0342452 , -0.02289275,  0.11516231,  0.46877548], dtype=float32), action=0, reward=1.0, next_state=array([-0.03470306, -0.21943721,  0.12453781,  0.7954223 ], dtype=float32), t=28, failed=False, limit=False),\n",
       "       SARS(state=array([-0.03470306, -0.21943721,  0.12453781,  0.7954223 ], dtype=float32), action=1, reward=1.0, next_state=array([-0.0390918 , -0.02622436,  0.14044626,  0.54436713], dtype=float32), t=29, failed=False, limit=False),\n",
       "       SARS(state=array([-0.0390918 , -0.02622436,  0.14044626,  0.54436713], dtype=float32), action=1, reward=1.0, next_state=array([-0.03961629,  0.16667366,  0.1513336 ,  0.29902476], dtype=float32), t=30, failed=False, limit=False),\n",
       "       SARS(state=array([-0.03961629,  0.16667366,  0.1513336 ,  0.29902476], dtype=float32), action=1, reward=1.0, next_state=array([-0.03628282,  0.35935098,  0.15731409,  0.0576344 ], dtype=float32), t=31, failed=False, limit=False),\n",
       "       SARS(state=array([-0.03628282,  0.35935098,  0.15731409,  0.0576344 ], dtype=float32), action=0, reward=1.0, next_state=array([-0.0290958 ,  0.16236423,  0.15846679,  0.39552563], dtype=float32), t=32, failed=False, limit=False),\n",
       "       SARS(state=array([-0.0290958 ,  0.16236423,  0.15846679,  0.39552563], dtype=float32), action=1, reward=1.0, next_state=array([-0.02584851,  0.35492462,  0.16637729,  0.15669861], dtype=float32), t=33, failed=False, limit=False),\n",
       "       SARS(state=array([-0.02584851,  0.35492462,  0.16637729,  0.15669861], dtype=float32), action=1, reward=1.0, next_state=array([-0.01875002,  0.5473221 ,  0.16951127, -0.0792229 ], dtype=float32), t=34, failed=False, limit=False),\n",
       "       SARS(state=array([-0.01875002,  0.5473221 ,  0.16951127, -0.0792229 ], dtype=float32), action=1, reward=1.0, next_state=array([-0.00780358,  0.73965985,  0.16792682, -0.3139964 ], dtype=float32), t=35, failed=False, limit=False),\n",
       "       SARS(state=array([-0.00780358,  0.73965985,  0.16792682, -0.3139964 ], dtype=float32), action=1, reward=1.0, next_state=array([ 0.00698962,  0.93204135,  0.16164689, -0.54937065], dtype=float32), t=36, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.00698962,  0.93204135,  0.16164689, -0.54937065], dtype=float32), action=1, reward=1.0, next_state=array([ 0.02563045,  1.1245676 ,  0.15065947, -0.7870779 ], dtype=float32), t=37, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.02563045,  1.1245676 ,  0.15065947, -0.7870779 ], dtype=float32), action=0, reward=1.0, next_state=array([ 0.0481218 ,  0.92773277,  0.13491791, -0.45104355], dtype=float32), t=38, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.0481218 ,  0.92773277,  0.13491791, -0.45104355], dtype=float32), action=0, reward=1.0, next_state=array([ 0.06667645,  0.7309864 ,  0.12589704, -0.11906037], dtype=float32), t=39, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.06667645,  0.7309864 ,  0.12589704, -0.11906037], dtype=float32), action=0, reward=1.0, next_state=array([0.08129618, 0.5343066 , 0.12351584, 0.21054047], dtype=float32), t=40, failed=False, limit=False),\n",
       "       SARS(state=array([0.08129618, 0.5343066 , 0.12351584, 0.21054047], dtype=float32), action=1, reward=1.0, next_state=array([ 0.09198231,  0.7274659 ,  0.12772664, -0.04076977], dtype=float32), t=41, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.09198231,  0.7274659 ,  0.12772664, -0.04076977], dtype=float32), action=1, reward=1.0, next_state=array([ 0.10653163,  0.9205469 ,  0.12691125, -0.2905824 ], dtype=float32), t=42, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.10653163,  0.9205469 ,  0.12691125, -0.2905824 ], dtype=float32), action=1, reward=1.0, next_state=array([ 0.12494257,  1.1136523 ,  0.1210996 , -0.5406991 ], dtype=float32), t=43, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.12494257,  1.1136523 ,  0.1210996 , -0.5406991 ], dtype=float32), action=1, reward=1.0, next_state=array([ 0.14721562,  1.3068826 ,  0.11028562, -0.79290545], dtype=float32), t=44, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.14721562,  1.3068826 ,  0.11028562, -0.79290545], dtype=float32), action=0, reward=1.0, next_state=array([ 0.17335327,  1.1104333 ,  0.09442751, -0.46766365], dtype=float32), t=45, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.17335327,  1.1104333 ,  0.09442751, -0.46766365], dtype=float32), action=0, reward=1.0, next_state=array([ 0.19556193,  0.91411304,  0.08507424, -0.14677462], dtype=float32), t=46, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.19556193,  0.91411304,  0.08507424, -0.14677462], dtype=float32), action=0, reward=1.0, next_state=array([0.2138442 , 0.71788234, 0.08213874, 0.17148855], dtype=float32), t=47, failed=False, limit=False),\n",
       "       SARS(state=array([0.2138442 , 0.71788234, 0.08213874, 0.17148855], dtype=float32), action=1, reward=1.0, next_state=array([ 0.22820185,  0.91173846,  0.08556851, -0.0941936 ], dtype=float32), t=48, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.22820185,  0.91173846,  0.08556851, -0.0941936 ], dtype=float32), action=0, reward=1.0, next_state=array([0.24643661, 0.71550095, 0.08368464, 0.22421217], dtype=float32), t=49, failed=False, limit=False),\n",
       "       SARS(state=array([0.24643661, 0.71550095, 0.08368464, 0.22421217], dtype=float32), action=0, reward=1.0, next_state=array([0.26074663, 0.51928884, 0.08816888, 0.5420749 ], dtype=float32), t=50, failed=False, limit=False),\n",
       "       SARS(state=array([0.26074663, 0.51928884, 0.08816888, 0.5420749 ], dtype=float32), action=1, reward=1.0, next_state=array([0.2711324 , 0.7130682 , 0.09901039, 0.2784231 ], dtype=float32), t=51, failed=False, limit=False),\n",
       "       SARS(state=array([0.2711324 , 0.7130682 , 0.09901039, 0.2784231 ], dtype=float32), action=0, reward=1.0, next_state=array([0.28539377, 0.5166834 , 0.10457885, 0.600619  ], dtype=float32), t=52, failed=False, limit=False),\n",
       "       SARS(state=array([0.28539377, 0.5166834 , 0.10457885, 0.600619  ], dtype=float32), action=1, reward=1.0, next_state=array([0.29572743, 0.7101989 , 0.11659123, 0.34262186], dtype=float32), t=53, failed=False, limit=False),\n",
       "       SARS(state=array([0.29572743, 0.7101989 , 0.11659123, 0.34262186], dtype=float32), action=1, reward=1.0, next_state=array([0.30993143, 0.90348583, 0.12344366, 0.08886005], dtype=float32), t=54, failed=False, limit=False),\n",
       "       SARS(state=array([0.30993143, 0.90348583, 0.12344366, 0.08886005], dtype=float32), action=0, reward=1.0, next_state=array([0.32800114, 0.70683056, 0.12522087, 0.41779864], dtype=float32), t=55, failed=False, limit=False),\n",
       "       SARS(state=array([0.32800114, 0.70683056, 0.12522087, 0.41779864], dtype=float32), action=0, reward=1.0, next_state=array([0.34213775, 0.5101772 , 0.13357684, 0.7471878 ], dtype=float32), t=56, failed=False, limit=False),\n",
       "       SARS(state=array([0.34213775, 0.5101772 , 0.13357684, 0.7471878 ], dtype=float32), action=1, reward=1.0, next_state=array([0.3523413 , 0.7032281 , 0.14852059, 0.49934587], dtype=float32), t=57, failed=False, limit=False),\n",
       "       SARS(state=array([0.3523413 , 0.7032281 , 0.14852059, 0.49934587], dtype=float32), action=1, reward=1.0, next_state=array([0.36640584, 0.89597845, 0.15850751, 0.25690806], dtype=float32), t=58, failed=False, limit=False),\n",
       "       SARS(state=array([0.36640584, 0.89597845, 0.15850751, 0.25690806], dtype=float32), action=1, reward=1.0, next_state=array([0.3843254 , 1.0885242 , 0.16364567, 0.01811639], dtype=float32), t=59, failed=False, limit=False),\n",
       "       SARS(state=array([0.3843254 , 1.0885242 , 0.16364567, 0.01811639], dtype=float32), action=1, reward=1.0, next_state=array([ 0.40609592,  1.2809672 ,  0.164008  , -0.21879418], dtype=float32), t=60, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.40609592,  1.2809672 ,  0.164008  , -0.21879418], dtype=float32), action=1, reward=1.0, next_state=array([ 0.43171525,  1.4734112 ,  0.15963212, -0.45558408], dtype=float32), t=61, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.43171525,  1.4734112 ,  0.15963212, -0.45558408], dtype=float32), action=0, reward=1.0, next_state=array([ 0.4611835 ,  1.276435  ,  0.15052043, -0.11714351], dtype=float32), t=62, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.4611835 ,  1.276435  ,  0.15052043, -0.11714351], dtype=float32), action=1, reward=1.0, next_state=array([ 0.4867122 ,  1.4691157 ,  0.14817756, -0.35881057], dtype=float32), t=63, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.4867122 ,  1.4691157 ,  0.14817756, -0.35881057], dtype=float32), action=0, reward=1.0, next_state=array([ 0.5160945 ,  1.2722322 ,  0.14100136, -0.02331658], dtype=float32), t=64, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.5160945 ,  1.2722322 ,  0.14100136, -0.02331658], dtype=float32), action=0, reward=1.0, next_state=array([0.54153913, 1.0753993 , 0.14053503, 0.3103198 ], dtype=float32), t=65, failed=False, limit=False),\n",
       "       SARS(state=array([0.54153913, 1.0753993 , 0.14053503, 0.3103198 ], dtype=float32), action=1, reward=1.0, next_state=array([0.5630471 , 1.2682685 , 0.14674142, 0.06504967], dtype=float32), t=66, failed=False, limit=False),\n",
       "       SARS(state=array([0.5630471 , 1.2682685 , 0.14674142, 0.06504967], dtype=float32), action=1, reward=1.0, next_state=array([ 0.58841246,  1.4610151 ,  0.14804241, -0.17797573], dtype=float32), t=67, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.58841246,  1.4610151 ,  0.14804241, -0.17797573], dtype=float32), action=0, reward=1.0, next_state=array([0.6176328 , 1.2641189 , 0.1444829 , 0.15750366], dtype=float32), t=68, failed=False, limit=False),\n",
       "       SARS(state=array([0.6176328 , 1.2641189 , 0.1444829 , 0.15750366], dtype=float32), action=0, reward=1.0, next_state=array([0.6429152 , 1.0672556 , 0.14763297, 0.49205202], dtype=float32), t=69, failed=False, limit=False),\n",
       "       SARS(state=array([0.6429152 , 1.0672556 , 0.14763297, 0.49205202], dtype=float32), action=1, reward=1.0, next_state=array([0.66426027, 1.2600205 , 0.15747401, 0.24929667], dtype=float32), t=70, failed=False, limit=False),\n",
       "       SARS(state=array([0.66426027, 1.2600205 , 0.15747401, 0.24929667], dtype=float32), action=1, reward=1.0, next_state=array([0.6894607 , 1.4525841 , 0.16245994, 0.01013141], dtype=float32), t=71, failed=False, limit=False),\n",
       "       SARS(state=array([0.6894607 , 1.4525841 , 0.16245994, 0.01013141], dtype=float32), action=1, reward=1.0, next_state=array([ 0.71851236,  1.6450486 ,  0.16266257, -0.22721036], dtype=float32), t=72, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.71851236,  1.6450486 ,  0.16266257, -0.22721036], dtype=float32), action=1, reward=1.0, next_state=array([ 0.75141335,  1.8375175 ,  0.15811837, -0.46449047], dtype=float32), t=73, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.75141335,  1.8375175 ,  0.15811837, -0.46449047], dtype=float32), action=1, reward=1.0, next_state=array([ 0.7881637 ,  2.0300932 ,  0.14882855, -0.7034572 ], dtype=float32), t=74, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.7881637 ,  2.0300932 ,  0.14882855, -0.7034572 ], dtype=float32), action=1, reward=1.0, next_state=array([ 0.8287656 ,  2.2228734 ,  0.13475941, -0.94583684], dtype=float32), t=75, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.8287656 ,  2.2228734 ,  0.13475941, -0.94583684], dtype=float32), action=1, reward=1.0, next_state=array([ 0.873223  ,  2.4159484 ,  0.11584268, -1.193324  ], dtype=float32), t=76, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.873223  ,  2.4159484 ,  0.11584268, -1.193324  ], dtype=float32), action=0, reward=1.0, next_state=array([ 0.921542  ,  2.2195325 ,  0.0919762 , -0.86669314], dtype=float32), t=77, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.921542  ,  2.2195325 ,  0.0919762 , -0.86669314], dtype=float32), action=1, reward=1.0, next_state=array([ 0.96593267,  2.4132905 ,  0.07464233, -1.1290988 ], dtype=float32), t=78, failed=False, limit=False),\n",
       "       SARS(state=array([ 0.96593267,  2.4132905 ,  0.07464233, -1.1290988 ], dtype=float32), action=1, reward=1.0, next_state=array([ 1.0141984 ,  2.6073596 ,  0.05206036, -1.3974676 ], dtype=float32), t=79, failed=False, limit=False),\n",
       "       SARS(state=array([ 1.0141984 ,  2.6073596 ,  0.05206036, -1.3974676 ], dtype=float32), action=1, reward=1.0, next_state=array([ 1.0663457,  2.801797 ,  0.024111 , -1.6734296], dtype=float32), t=80, failed=False, limit=False),\n",
       "       SARS(state=array([ 1.0663457,  2.801797 ,  0.024111 , -1.6734296], dtype=float32), action=0, reward=1.0, next_state=array([ 1.1223816 ,  2.6064036 , -0.00935759, -1.3733367 ], dtype=float32), t=81, failed=False, limit=False),\n",
       "       SARS(state=array([ 1.1223816 ,  2.6064036 , -0.00935759, -1.3733367 ], dtype=float32), action=0, reward=1.0, next_state=array([ 1.1745096 ,  2.4113998 , -0.03682432, -1.0835949 ], dtype=float32), t=82, failed=False, limit=False),\n",
       "       SARS(state=array([ 1.1745096 ,  2.4113998 , -0.03682432, -1.0835949 ], dtype=float32), action=0, reward=1.0, next_state=array([ 1.2227377 ,  2.2167826 , -0.05849622, -0.802691  ], dtype=float32), t=83, failed=False, limit=False),\n",
       "       SARS(state=array([ 1.2227377 ,  2.2167826 , -0.05849622, -0.802691  ], dtype=float32), action=1, reward=1.0, next_state=array([ 1.2670733 ,  2.4126558 , -0.07455004, -1.1131864 ], dtype=float32), t=84, failed=False, limit=False),\n",
       "       SARS(state=array([ 1.2670733 ,  2.4126558 , -0.07455004, -1.1131864 ], dtype=float32), action=1, reward=1.0, next_state=array([ 1.3153265 ,  2.6086736 , -0.09681377, -1.4282935 ], dtype=float32), t=85, failed=False, limit=False),\n",
       "       SARS(state=array([ 1.3153265 ,  2.6086736 , -0.09681377, -1.4282935 ], dtype=float32), action=0, reward=1.0, next_state=array([ 1.3675    ,  2.4148715 , -0.12537964, -1.1673704 ], dtype=float32), t=86, failed=False, limit=False),\n",
       "       SARS(state=array([ 1.3675    ,  2.4148715 , -0.12537964, -1.1673704 ], dtype=float32), action=1, reward=1.0, next_state=array([ 1.4157974 ,  2.6113815 , -0.14872704, -1.496587  ], dtype=float32), t=87, failed=False, limit=False),\n",
       "       SARS(state=array([ 1.4157974 ,  2.6113815 , -0.14872704, -1.496587  ], dtype=float32), action=1, reward=1.0, next_state=array([ 1.468025  ,  2.8079655 , -0.17865878, -1.8317724 ], dtype=float32), t=88, failed=False, limit=False),\n",
       "       SARS(state=array([ 1.468025  ,  2.8079655 , -0.17865878, -1.8317724 ], dtype=float32), action=0, reward=1.0, next_state=array([ 1.5241842 ,  2.6152153 , -0.21529423, -1.5994958 ], dtype=float32), t=89, failed=True, limit=False)])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88986d2",
   "metadata": {},
   "source": [
    "# Polyak Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "131ea41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4083,  0.0696, -0.2966, -0.0051],\n",
       "        [ 0.0843,  0.1779, -0.2049,  0.4490],\n",
       "        [ 0.0683, -0.1412, -0.4319, -0.0770],\n",
       "        ...,\n",
       "        [-0.0125, -0.0770,  0.1609, -0.4340],\n",
       "        [-0.0757,  0.0085,  0.4197, -0.1831],\n",
       "        [-0.3773, -0.4425,  0.2319,  0.0120]], requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_parameter_1 = next(policy.policy_network.named_parameters())[1]\n",
    "test_parameter_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52883c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0128, 0.0128, 0.0128, 0.0128],\n",
       "        [0.0128, 0.0128, 0.0128, 0.0128],\n",
       "        [0.0128, 0.0128, 0.0128, 0.0128],\n",
       "        ...,\n",
       "        [0.0128, 0.0128, 0.0128, 0.0128],\n",
       "        [0.0128, 0.0128, 0.0128, 0.0128],\n",
       "        [0.0128, 0.0128, 0.0128, 0.0128]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_parameter_2 = test_parameter_1 * 0 + 0.0128\n",
    "test_parameter_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f54a772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3662,  0.0639, -0.2657, -0.0033],\n",
       "        [ 0.0772,  0.1614, -0.1831,  0.4054],\n",
       "        [ 0.0627, -0.1258, -0.3874, -0.0680],\n",
       "        ...,\n",
       "        [-0.0100, -0.0680,  0.1461, -0.3893],\n",
       "        [-0.0669,  0.0090,  0.3790, -0.1635],\n",
       "        [-0.3382, -0.3969,  0.2100,  0.0121]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_parameter_1 * 0.9 + test_parameter_2 * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "149ba5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyak_update(network_to_update, target_network, tau=0.001):\n",
    "    with torch.no_grad():\n",
    "        for to_update, target in zip(network_to_update.parameters(), target_network.parameters()):\n",
    "            to_update *= 1-tau\n",
    "            to_update += target * tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e94d921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4177, -0.2698,  0.1713, -0.4210, -0.0153],\n",
       "        [ 0.0030,  0.2139,  0.3215, -0.0469, -0.0017],\n",
       "        [ 0.4171,  0.0285,  0.3802, -0.0684,  0.0761],\n",
       "        ...,\n",
       "        [-0.4030, -0.0351, -0.2655, -0.3643,  0.0377],\n",
       "        [ 0.2405, -0.3039,  0.0783,  0.3646,  0.0467],\n",
       "        [-0.2339,  0.0512, -0.1346, -0.3551, -0.3111]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3832, -0.1674, -0.2559, -0.2801,  0.0834],\n",
       "        [ 0.1957,  0.1690,  0.0934, -0.2222, -0.3882],\n",
       "        [ 0.4304, -0.0352,  0.2414,  0.0199, -0.4374],\n",
       "        ...,\n",
       "        [-0.4034, -0.2170, -0.3155,  0.0643, -0.1919],\n",
       "        [ 0.2143,  0.2310, -0.1604,  0.3832,  0.0863],\n",
       "        [-0.3501, -0.1050,  0.0482, -0.3175, -0.2855]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4177, -0.2698,  0.1713, -0.4210, -0.0153],\n",
       "        [ 0.0030,  0.2139,  0.3215, -0.0469, -0.0017],\n",
       "        [ 0.4171,  0.0285,  0.3802, -0.0684,  0.0761],\n",
       "        ...,\n",
       "        [-0.4030, -0.0351, -0.2655, -0.3643,  0.0377],\n",
       "        [ 0.2405, -0.3039,  0.0783,  0.3646,  0.0467],\n",
       "        [-0.2339,  0.0512, -0.1346, -0.3551, -0.3111]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3867, -0.1776, -0.2132, -0.2942,  0.0735],\n",
       "        [ 0.1764,  0.1735,  0.1162, -0.2047, -0.3495],\n",
       "        [ 0.4291, -0.0288,  0.2553,  0.0110, -0.3861],\n",
       "        ...,\n",
       "        [-0.4034, -0.1988, -0.3105,  0.0215, -0.1690],\n",
       "        [ 0.2169,  0.1775, -0.1365,  0.3814,  0.0823],\n",
       "        [-0.3385, -0.0893,  0.0299, -0.3212, -0.2881]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_network_1 = QNetwork(5, 3)\n",
    "test_network_2 = QNetwork(5, 3)\n",
    "display(list(test_network_1.parameters())[0])\n",
    "display(list(test_network_2.parameters())[0])\n",
    "polyak_update(test_network_2, test_network_1, 0.1)\n",
    "display(list(test_network_1.parameters())[0])\n",
    "display(list(test_network_2.parameters())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936a6271",
   "metadata": {},
   "source": [
    "# Log Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d9242c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.01005034, 4.60517019])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4.615220521841592"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.10536052, 2.30258509])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2.4079456086518722"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.22314355, 1.60943791])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.8325814637483102"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.51082562, 0.91629073])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.4271163556401456"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.69314718, 0.69314718])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.3862943611198906"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for p in [0.99, 0.9, 0.8, 0.6, 0.5]:\n",
    "    logs = -np.log([p, 1-p])\n",
    "    display(p, logs, sum(logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e37d9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SARS(state=array([-0.02486775, -0.01806459,  0.07174376,  0.36024594], dtype=float32), action=0, reward=1.0, next_state=array([-0.02522904, -0.21412914,  0.07894868,  0.6746608 ], dtype=float32), t=24, failed=False, limit=False),\n",
       " SARS(state=array([-0.03470306, -0.21943721,  0.12453781,  0.7954223 ], dtype=float32), action=1, reward=1.0, next_state=array([-0.0390918 , -0.02622436,  0.14044626,  0.54436713], dtype=float32), t=29, failed=False, limit=False),\n",
       " SARS(state=array([ 0.01308193, -0.00113932, -0.02612523, -0.01392275], dtype=float32), action=0, reward=1.0, next_state=array([ 0.01305915, -0.19587706, -0.02640369,  0.27040422], dtype=float32), t=2, failed=False, limit=False),\n",
       " SARS(state=array([ 0.14721562,  1.3068826 ,  0.11028562, -0.79290545], dtype=float32), action=0, reward=1.0, next_state=array([ 0.17335327,  1.1104333 ,  0.09442751, -0.46766365], dtype=float32), t=45, failed=False, limit=False)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(replay_buffer, k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4199269",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "![Psudocode](sac_psudocode.png)\n",
    "\n",
    "Source: https://spinningup.openai.com/en/latest/algorithms/sac.html#pseudocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70fb9529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_min(q1, q2, states, actions):\n",
    "    \n",
    "    def f(q):\n",
    "        state_values = q.forward(states).detach()\n",
    "        chosen_action_values = torch.sum(state_values * nn.functional.one_hot(actions), 1)\n",
    "        return chosen_action_values\n",
    "        \n",
    "    return torch.minimum(*map(f, (q1, q2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "856452ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train/q_loss_1': tensor(1.0747, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " 'train/q_loss_2': tensor(1.0940, dtype=torch.float64, grad_fn=<MeanBackward0>),\n",
       " 'train/policy_loss': tensor(-0.0517, grad_fn=<MulBackward0>)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    stats = {}\n",
    "    # Step 11\n",
    "    training_batch = random.sample(replay_buffer, k=min(len(replay_buffer), 100))\n",
    "    # Prep\n",
    "    states = torch.tensor(np.array([sars.state for sars in training_batch]), requires_grad=False)\n",
    "    actions = torch.tensor(np.array([sars.action for sars in training_batch]), requires_grad=False)\n",
    "    actions_hot = nn.functional.one_hot(actions)\n",
    "    rewards = torch.tensor(np.array([sars.reward for sars in training_batch]), requires_grad=False)\n",
    "    next_states = torch.tensor(np.array([sars.next_state for sars in training_batch]), requires_grad=False)\n",
    "    fails = torch.tensor(np.array([sars.failed for sars in training_batch]), dtype=int, requires_grad=False)\n",
    "    # Step 12\n",
    "    next_action_probs = policy.policy_network.forward(next_states).detach()\n",
    "    assert not next_action_probs.requires_grad\n",
    "    next_actions_categorical = torch.distributions.Categorical(next_action_probs)\n",
    "    next_actions = next_actions_categorical.sample()\n",
    "    assert not next_actions.requires_grad\n",
    "    next_actions_q_min = q_min(\n",
    "        policy.q1_target_network,\n",
    "        policy.q2_target_network,\n",
    "        next_states,\n",
    "        next_actions)\n",
    "    assert not next_actions_q_min.requires_grad\n",
    "    next_actions_log_probs = next_actions_categorical.log_prob(next_actions)\n",
    "    assert not next_actions_log_probs.requires_grad\n",
    "    y = rewards + GAMMA * (1-fails) * (next_actions_q_min - ALPHA * next_actions_log_probs)\n",
    "    assert not y.requires_grad\n",
    "    # Step 13\n",
    "    for qi, q, opt in ((1, policy.q1_network, policy.q1_optimizer),\n",
    "                       (2, policy.q2_network, policy.q2_optimizer)):\n",
    "        assert not states.requires_grad\n",
    "        assert not actions_hot.requires_grad\n",
    "        q_state_action = torch.sum(q.forward(states) * actions_hot, 1)\n",
    "        assert q_state_action.requires_grad\n",
    "        q_loss = torch.mean((q_state_action - y)**2)\n",
    "        stats[f'train/q_loss_{qi}'] = q_loss\n",
    "        assert q_loss.requires_grad\n",
    "        opt.zero_grad()\n",
    "        q_loss.backward()\n",
    "        opt.step()\n",
    "    # Step 14\n",
    "    action_probs = policy.policy_network.forward(states)\n",
    "    assert action_probs.requires_grad\n",
    "    actions_categorical = torch.distributions.Categorical(action_probs)\n",
    "    actions = actions_categorical.sample()\n",
    "    assert not actions.requires_grad\n",
    "    actions_q_min = q_min(\n",
    "        policy.q1_network,\n",
    "        policy.q2_network,\n",
    "        states,\n",
    "        actions)\n",
    "    assert not actions_q_min.requires_grad\n",
    "    actions_log_probs = actions_categorical.log_prob(actions)\n",
    "    assert actions_log_probs.requires_grad\n",
    "    policy_loss = -1 * torch.mean(actions_q_min - ALPHA * actions_log_probs)\n",
    "    stats['train/policy_loss'] = policy_loss\n",
    "    policy.policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy.policy_optimizer.step()\n",
    "    # Step 15\n",
    "    polyak_update(policy.q1_target_network, policy.q1_network, tau=TAU)\n",
    "    polyak_update(policy.q2_target_network, policy.q2_network, tau=TAU)\n",
    "    stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a96cd6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(policy, replay_buffer):\n",
    "    stats = {}\n",
    "    # Step 11\n",
    "    training_batch = random.sample(replay_buffer, k=min(len(replay_buffer), 100))\n",
    "    # Prep\n",
    "    states = torch.tensor(np.array([sars.state for sars in training_batch]), requires_grad=False)\n",
    "    actions = torch.tensor(np.array([sars.action for sars in training_batch]), requires_grad=False)\n",
    "    actions_hot = nn.functional.one_hot(actions)\n",
    "    rewards = torch.tensor(np.array([sars.reward for sars in training_batch]), requires_grad=False)\n",
    "    next_states = torch.tensor(np.array([sars.next_state for sars in training_batch]), requires_grad=False)\n",
    "    fails = torch.tensor(np.array([sars.failed for sars in training_batch]), dtype=int, requires_grad=False)\n",
    "    # Step 12\n",
    "    next_action_probs = policy.policy_network.forward(next_states).detach()\n",
    "    assert not next_action_probs.requires_grad\n",
    "    next_actions_categorical = torch.distributions.Categorical(next_action_probs)\n",
    "    next_actions = next_actions_categorical.sample()\n",
    "    assert not next_actions.requires_grad\n",
    "    next_actions_q_min = q_min(\n",
    "        policy.q1_target_network,\n",
    "        policy.q2_target_network,\n",
    "        next_states,\n",
    "        next_actions)\n",
    "    assert not next_actions_q_min.requires_grad\n",
    "    next_actions_log_probs = next_actions_categorical.log_prob(next_actions)\n",
    "    assert not next_actions_log_probs.requires_grad\n",
    "    y = rewards + GAMMA * (1-fails) * (next_actions_q_min - ALPHA * next_actions_log_probs)\n",
    "    assert not y.requires_grad\n",
    "    # Step 13\n",
    "    for qi, q, opt in ((1, policy.q1_network, policy.q1_optimizer),\n",
    "                       (2, policy.q2_network, policy.q2_optimizer)):\n",
    "        assert not states.requires_grad\n",
    "        assert not actions_hot.requires_grad\n",
    "        q_state_action = torch.sum(q.forward(states) * actions_hot, 1)\n",
    "        assert q_state_action.requires_grad\n",
    "        q_loss = torch.mean((q_state_action - y)**2)\n",
    "        stats[f'train/q_loss_{qi}'] = q_loss\n",
    "        assert q_loss.requires_grad\n",
    "        opt.zero_grad()\n",
    "        q_loss.backward()\n",
    "        opt.step()\n",
    "    # Step 14\n",
    "    action_probs = policy.policy_network.forward(states)\n",
    "    assert action_probs.requires_grad\n",
    "    actions_categorical = torch.distributions.Categorical(action_probs)\n",
    "    actions = actions_categorical.sample()\n",
    "    assert not actions.requires_grad\n",
    "    actions_q_min = q_min(\n",
    "        policy.q1_network,\n",
    "        policy.q2_network,\n",
    "        states,\n",
    "        actions)\n",
    "    assert not actions_q_min.requires_grad\n",
    "    actions_log_probs = actions_categorical.log_prob(actions)\n",
    "    assert actions_log_probs.requires_grad\n",
    "    policy_loss = -1 * torch.mean(actions_q_min - ALPHA * actions_log_probs)\n",
    "    stats['train/policy_loss'] = policy_loss\n",
    "    policy.policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy.policy_optimizer.step()\n",
    "    # Step 15\n",
    "    polyak_update(policy.q1_target_network, policy.q1_network, tau=TAU)\n",
    "    polyak_update(policy.q2_target_network, policy.q2_network, tau=TAU)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b0a3248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|                                                                      | 385/1000 [03:13<05:09,  1.99it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m tb_writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain/replay_buffer_length\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(replay_buffer), episode)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m training_iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 16\u001b[0m     stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stat, value \u001b[38;5;129;01min\u001b[39;00m stats\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     18\u001b[0m         tb_writer\u001b[38;5;241m.\u001b[39madd_scalar(stat, value, episode)\n",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(policy, replay_buffer)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m q_loss\u001b[38;5;241m.\u001b[39mrequires_grad\n\u001b[1;32m     38\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 39\u001b[0m     \u001b[43mq_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Step 14\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/python/learn-pytorch/.venv/lib64/python3.10/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/python/learn-pytorch/.venv/lib64/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tb_writer = SummaryWriter()\n",
    "\n",
    "oss = env.observation_space.shape\n",
    "if len(oss) != 1:\n",
    "    raise RuntimeError(f'Unknown observation_space.shape: {oss}')\n",
    "os_len = oss[0]\n",
    "policy = Policy(os_len, env.action_space.n)\n",
    "\n",
    "replay_buffer = deque(maxlen=10_000)\n",
    "\n",
    "for episode in tqdm.tqdm(range(1, 1000+1)):\n",
    "    episode_reward = run_episode(action, step, env, policy)\n",
    "    tb_writer.add_scalar('main/episode_reward', episode_reward, episode)\n",
    "    tb_writer.add_scalar('main/replay_buffer_length', len(replay_buffer), episode)\n",
    "    for training_iteration in range(1, 100+1):\n",
    "        stats = train(policy, replay_buffer)\n",
    "        for stat, value in stats.items():\n",
    "            tb_writer.add_scalar(stat, value, episode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
